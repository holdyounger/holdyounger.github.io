

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme="dark">



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="https://avatars.githubusercontent.com/u/88082435?v=4">
  <link rel="icon" href="https://avatars.githubusercontent.com/u/88082435?v=4">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="mingming">
  <meta name="keywords" content>
  
    <meta name="description" content="概述: 实验楼机器学习基础">
<meta property="og:type" content="article">
<meta property="og:title" content="【机器学习基础】导读">
<meta property="og:url" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/index.html">
<meta property="og:site_name" content="oone">
<meta property="og:description" content="概述: 实验楼机器学习基础">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175317777.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175318456.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175319246.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175320313.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175320995.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175321442.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100620528.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175323490.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175324783.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175325739.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175327325.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175328060.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175329058.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100621566.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100621716.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622196.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622282.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175335239.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175335997.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622578.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622695.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622792.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175339427.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622978.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175342146.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100623178.png">
<meta property="og:image" content="http://example.com/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100623265.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175344202.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175345600.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175346295.png">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175347318.svg+xml">
<meta property="og:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175347669.gif">
<meta property="article:published_time" content="2024-06-17T10:02:42.554Z">
<meta property="article:modified_time" content="2024-08-26T09:53:48.002Z">
<meta property="article:author" content="mingming">
<meta property="article:tag" content="实验楼">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="machine-learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175317777.png">
  
  
  
  <title>【机器学习基础】导读 - oone</title>

  <link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css">



  <link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link rel="stylesheet" href="/css/main.css">


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css">
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css">
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":"holdyounger.github.io"}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script src="/js/utils.js"></script>
  <script src="/js/color-schema.js"></script>
  


  
<meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="oone" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Montarius</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax="true" style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【机器学习基础】导读"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-06-17 18:02" pubdate>
          2024年6月17日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          30k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          248 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【机器学习基础】导读</h1>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>概述: 实验楼机器学习基础</p>
</blockquote>
<span id="more"></span>

<h2 id="使用-Pandas-进行数据探索"><a href="#使用-Pandas-进行数据探索" class="headerlink" title="使用 Pandas 进行数据探索"></a>使用 Pandas 进行数据探索</h2><hr>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>本次实验通过分析电信运营商的客户离网率数据集来熟悉 Pandas 数据探索的常用方法，并构建一个预测客户离网率的简单模型。</p>
<h4 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h4><ul>
<li>排列</li>
<li>索引</li>
<li>交叉表</li>
<li>透视表</li>
<li>数据探索</li>
</ul>
<hr>
<p>课程介绍</p>
<p>机器学习开放基础课程是蓝桥云课经由 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://mlcourse.ai/"> <em>Open Machine Learning Course</em></a> 授权并制作的机器学习免费基础实战课。蓝桥云课和 mlcourse.ai 共同享有改编内容版权。</p>
<p>我们在原英文课程提供的内容和代码基础上进行了编译，并不局限于简单的翻译。特别地，我们针对部分代码和内容进行增删，添加了更多的讲解和注释性内容，内容也更易于国内用户理解。另外，课程适配了蓝桥云课提供的线上 Notebook 实验环境，让你可以随时随地开始学习。该课程适合于对机器学习感兴趣的用户，但需要具备基础的 Python 编程能力和数学水平。</p>
<p>如果你学习完该课程后，想进一步深入学习机器学习或数据分析知识，欢迎报名 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/louplus/"> <em>蓝桥云课精心打造的楼+ 课程</em></a>。</p>
<h3 id="Pandas-的主要方法"><a href="#Pandas-的主要方法" class="headerlink" title="Pandas 的主要方法"></a>Pandas 的主要方法</h3><p>Pandas 是基于 NumPy 的一种工具，提供了大量数据探索的方法。Pandas 可以使用类似 SQL 的方式对 .csv、.tsv、.xlsx 等格式的数据进行处理分析。</p>
<p>Pandas 主要使用的数据结构是 Series 和 DataFrame 类。下面简要介绍下这两类：</p>
<ul>
<li>Series 是一种类似于一维数组的对象，它由一组数据（各种 NumPy 数据类型）及一组与之相关的数据标签（即索引）组成。</li>
<li>DataFrame 是一个二维数据结构，即一张表格，其中每列数据的类型相同。你可以把它看成由 Series 实例构成的字典。</li>
</ul>
<p>下面开始此次实验，我们将通过分析电信运营商的客户离网率数据集来展示 Pandas 的主要方法。</p>
<p>首先载入必要的库，即 NumPy 和 Pandas。</p>
<p> <em>教学代码：</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p> <em>动手练习</em>｜如果你对课程所使用的蓝桥云课 Notebook 在线环境并不熟悉，可以先学习 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/courses/1322"> <em>使用指南课程</em></a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在此空白单元格中对照教学代码练习即可</span><br></code></pre></td></tr></table></figure>

<p>通过 <code>read_csv()</code> 方法读取数据，然后使用 <code>head()</code> 方法查看前 5 行数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.read_csv(<br>    <span class="hljs-string">&#x27;https://labfile.oss.aliyuncs.com/courses/1283/telecom_churn.csv&#x27;</span>)<br>df.head()<br></code></pre></td></tr></table></figure>

<p>上图中的每行对应一位客户，每列对应客户的一个特征。</p>
<p>让我们查看一下该数据库的维度、特征名称和特征类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.shape<br></code></pre></td></tr></table></figure>

<p>上述结果表明，我们的列表包含 3333 行和 20 列。下面我们尝试打印列名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.columns<br></code></pre></td></tr></table></figure>

<p>我们还可以使用 <code>info()</code> 方法输出 DataFrame 的一些总体信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.info()<br></code></pre></td></tr></table></figure>

<p><code>bool</code>、<code>int64</code>、<code>float64</code> 和 <code>object</code> 是该数据库特征的数据类型。这一方法同时也会显示是否有缺失值，上述结果表明在该数据集中不存在缺失值，因为每列都包含 3333 个观测，和我们之前使用 <code>shape</code> 方法得到的数字是一致的。</p>
<p><code>astype()</code> 方法可以更改列的类型，下列公式将 Churn 离网率 特征修改为 <code>int64</code> 类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Churn&#x27;</span>] = df[<span class="hljs-string">&#x27;Churn&#x27;</span>].astype(<span class="hljs-string">&#x27;int64&#x27;</span>)<br><br></code></pre></td></tr></table></figure>

<p><code>describe()</code> 方法可以显示数值特征（<code>int64</code> 和 <code>float64</code>）的基本统计学特性，如未缺失值的数值、均值、标准差、范围、四分位数等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.describe()<br></code></pre></td></tr></table></figure>

<p>通过 include 参数显式指定包含的数据类型，可以查看非数值特征的统计数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.describe(include=[<span class="hljs-string">&#x27;object&#x27;</span>, <span class="hljs-string">&#x27;bool&#x27;</span>])<br></code></pre></td></tr></table></figure>

<p><code>value_counts()</code> 方法可以查看类别（类型为 object ）和布尔值（类型为 bool ）特征。让我们看下 Churn 离网率 的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Churn&#x27;</span>].value_counts()<br></code></pre></td></tr></table></figure>

<p>上述结果表明，在 3333 位客户中， 2850 位是忠实客户，他们的 <code>Churn</code> 值为 0。调用 <code>value_counts()</code> 函数时，加上 <code>normalize=True</code> 参数可以显示比例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Churn&#x27;</span>].value_counts(normalize=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p>DataFrame 可以根据某个变量的值（也就是列）排序。比如，根据每日消费额排序（设置 ascending&#x3D;False 倒序排列）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.sort_values(by=<span class="hljs-string">&#x27;Total day charge&#x27;</span>, ascending=<span class="hljs-literal">False</span>).head()<br></code></pre></td></tr></table></figure>

<p>此外，还可以根据多个列的数值排序。下面函数实现的功能为：先按 Churn 离网率 升序排列，再按 Total day charge 每日总话费 降序排列，优先级 Churn &gt; Tatal day charge。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df.sort_values(by=[<span class="hljs-string">&#x27;Churn&#x27;</span>, <span class="hljs-string">&#x27;Total day charge&#x27;</span>],<br>               ascending=[<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]).head()<br></code></pre></td></tr></table></figure>

<h3 id="索引和获取数据"><a href="#索引和获取数据" class="headerlink" title="索引和获取数据"></a>索引和获取数据</h3><p>DataFrame 可以以不同的方式进行索引。</p>
<p>使用 <code>DataFrame[&#39;Name&#39;]</code> 可以得到一个单独的列。比如，离网率有多高？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Churn&#x27;</span>].mean()<br></code></pre></td></tr></table></figure>

<p>对一家公司而言，14.5% 的离网率是一个很糟糕的数据，这么高的离网率可能导致公司破产。</p>
<p>布尔值索引同样很方便，语法是 <code>df[P(df[&#39;Name&#39;])]</code>，P 是在检查 Name 列每个元素时所使用的逻辑条件。这一索引的输出是 DataFrame 的 Name 列中满足 P 条件的行。</p>
<p>让我们使用布尔值索引来回答这样以下问题：离网用户的数值变量的均值是多少？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[df[<span class="hljs-string">&#x27;Churn&#x27;</span>] == <span class="hljs-number">1</span>].mean()<br></code></pre></td></tr></table></figure>

<p>离网用户在白天打电话的总时长的均值是多少？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[df[<span class="hljs-string">&#x27;Churn&#x27;</span>] == <span class="hljs-number">1</span>][<span class="hljs-string">&#x27;Total day minutes&#x27;</span>].mean()<br></code></pre></td></tr></table></figure>

<p>未使用国际套餐（<code>International plan == NO</code>）的忠实用户（<code>Churn == 0</code>）所打的最长的国际长途是多久？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df[(df[<span class="hljs-string">&#x27;Churn&#x27;</span>] == <span class="hljs-number">0</span>) &amp; (df[<span class="hljs-string">&#x27;International plan&#x27;</span>] == <span class="hljs-string">&#x27;No&#x27;</span>)<br>   ][<span class="hljs-string">&#x27;Total intl minutes&#x27;</span>].<span class="hljs-built_in">max</span>()<br></code></pre></td></tr></table></figure>

<p>DataFrame 可以通过列名、行名、行号进行索引。<code>loc</code> 方法为通过名称索引，<code>iloc</code> 方法为通过数字索引。</p>
<p>通过 <code>loc</code> 方法输出 0 至 5 行、State 州 至 Area code 区号 的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.loc[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>, <span class="hljs-string">&#x27;State&#x27;</span>:<span class="hljs-string">&#x27;Area code&#x27;</span>]<br></code></pre></td></tr></table></figure>

<p>通过 <code>iloc</code> 方法输出前 5 行的前 3 列数据（和典型的 Python 切片一样，不含最大值）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>, <span class="hljs-number">0</span>:<span class="hljs-number">3</span>]<br></code></pre></td></tr></table></figure>

<p><code>df[:1]</code> 和 <code>df[-1:]</code> 可以得到 DataFrame 的首行和末行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[-<span class="hljs-number">1</span>:]<br></code></pre></td></tr></table></figure>

<h3 id="应用函数到单元格、列、行"><a href="#应用函数到单元格、列、行" class="headerlink" title="应用函数到单元格、列、行"></a>应用函数到单元格、列、行</h3><p>下面通过 <code>apply()</code> 方法应用函数 <code>max</code> 至每一列，即输出每列的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.apply(np.<span class="hljs-built_in">max</span>)<br></code></pre></td></tr></table></figure>

<p><code>apply()</code> 方法也可以应用函数至每一行，指定 axis&#x3D;1 即可。在这种情况下，使用 <code>lambda</code> 函数十分方便。比如，下面函数选中了所有以 W 开头的州。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[df[<span class="hljs-string">&#x27;State&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> state: state[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;W&#x27;</span>)].head()<br></code></pre></td></tr></table></figure>

<p><code>map()</code> 方法可以通过一个 {old_value:new_value} 形式的字典替换某一列中的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">d = &#123;<span class="hljs-string">&#x27;No&#x27;</span>: <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;Yes&#x27;</span>: <span class="hljs-literal">True</span>&#125;<br>df[<span class="hljs-string">&#x27;International plan&#x27;</span>] = df[<span class="hljs-string">&#x27;International plan&#x27;</span>].<span class="hljs-built_in">map</span>(d)<br>df.head()<br></code></pre></td></tr></table></figure>

<p>当然，使用 <code>repalce()</code> 方法一样可以达到替换的目的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df = df.replace(&#123;<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>: d&#125;)<br>df.head()<br></code></pre></td></tr></table></figure>

<h3 id="分组（Groupby）"><a href="#分组（Groupby）" class="headerlink" title="分组（Groupby）"></a>分组（Groupby）</h3><p>Pandas 下分组数据的一般形式为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.groupby(by=grouping_columns)[columns_to_show].function()<br></code></pre></td></tr></table></figure>

<p>对上述函数的解释：</p>
<ul>
<li><code>groupby()</code> 方法根据 grouping_columns 的值进行分组。</li>
<li>接着，选中感兴趣的列（columns_to_show）。若不包括这一项，那么就会选中所有非 groupby 列（即除 grouping_colums 外的所有列）。</li>
<li>最后，应用一个或多个函数（function）。</li>
</ul>
<p>在下面的例子中，我们根据 Churn 离网率 变量的值对数据进行分组，显示每组的统计数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">columns_to_show = [<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, <span class="hljs-string">&#x27;Total eve minutes&#x27;</span>,<br>                   <span class="hljs-string">&#x27;Total night minutes&#x27;</span>]<br><br>df.groupby([<span class="hljs-string">&#x27;Churn&#x27;</span>])[columns_to_show].describe(percentiles=[])<br></code></pre></td></tr></table></figure>

<p>和上面的例子类似，只不过这次将一些函数传给 <code>agg()</code>，通过 <code>agg()</code> 方法对分组后的数据进行聚合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">columns_to_show = [<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, <span class="hljs-string">&#x27;Total eve minutes&#x27;</span>,<br>                   <span class="hljs-string">&#x27;Total night minutes&#x27;</span>]<br><br>df.groupby([<span class="hljs-string">&#x27;Churn&#x27;</span>])[columns_to_show].agg([np.mean, np.std, np.<span class="hljs-built_in">min</span>, np.<span class="hljs-built_in">max</span>])<br></code></pre></td></tr></table></figure>

<h3 id="汇总表"><a href="#汇总表" class="headerlink" title="汇总表"></a>汇总表</h3><p>Pandas 中的透视表定义如下：</p>
<blockquote>
<p>透视表(Pivot Table)是电子表格程序和其他数据探索软件中一种常见的数据汇总工具。它根据一个或多个键对数据进行聚合，并根据行和列上的分组将数据分配到各个矩形区域中。</p>
</blockquote>
<p>通过 <code>pivot_table()</code> 方法可以建立透视表，其参数如下：</p>
<ul>
<li>values 表示需要计算的统计数据的变量列表</li>
<li>index 表示分组数据的变量列表</li>
<li>aggfunc 表示需要计算哪些统计数据，例如，总和、均值、最大值、最小值等。</li>
</ul>
<p>现在，通过 <code>pivot_table()</code> 方法查看不同区号下白天、夜晚、深夜的电话量的均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df.pivot_table([<span class="hljs-string">&#x27;Total day calls&#x27;</span>, <span class="hljs-string">&#x27;Total eve calls&#x27;</span>, <span class="hljs-string">&#x27;Total night calls&#x27;</span>],<br>               [<span class="hljs-string">&#x27;Area code&#x27;</span>], aggfunc=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><code>pivot_table()</code> 其他的使用方法见 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/courses/1091/labs/6138/document"> <em>Pandas 百题大冲关</em></a> 的透视表部分。</p>
<p>交叉表（Cross Tabulation）是一种用于计算分组频率的特殊透视表，在 Pandas 中一般使用 <code>crosstab()</code> 方法构建交叉表。</p>
<p>构建一个交叉表查看样本的 Churn 离网率 和 International plan 国际套餐 的分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.crosstab(df[<span class="hljs-string">&#x27;Churn&#x27;</span>], df[<span class="hljs-string">&#x27;International plan&#x27;</span>])<br></code></pre></td></tr></table></figure>

<p>构建一个交叉表查看 Churn 离网率 和 Voice mail plan 语音邮件套餐 的分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.crosstab(df[<span class="hljs-string">&#x27;Churn&#x27;</span>], df[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>], normalize=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>上述结果表明，大部分用户是忠实用户，同时他们并不使用额外的服务（国际套餐、语音邮件）。</p>
<h3 id="增减-DataFrame-的行列"><a href="#增减-DataFrame-的行列" class="headerlink" title="增减 DataFrame 的行列"></a>增减 DataFrame 的行列</h3><p>在 DataFrame 中新增列有很多方法，比如，使用 <code>insert()</code>方法添加列，为所有用户计算总的 Total calls 电话量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">total_calls = df[<span class="hljs-string">&#x27;Total day calls&#x27;</span>] + df[<span class="hljs-string">&#x27;Total eve calls&#x27;</span>] + \<br>    df[<span class="hljs-string">&#x27;Total night calls&#x27;</span>] + df[<span class="hljs-string">&#x27;Total intl calls&#x27;</span>]<br><span class="hljs-comment"># loc 参数是插入 Series 对象后选择的列数</span><br><span class="hljs-comment"># 设置为 len(df.columns)以便将计算后的 Total calls 粘贴到最后一列</span><br>df.insert(loc=<span class="hljs-built_in">len</span>(df.columns), column=<span class="hljs-string">&#x27;Total calls&#x27;</span>, value=total_calls)<br><br>df.head()<br></code></pre></td></tr></table></figure>

<p>上面的代码创建了一个中间 Series 实例，即 total_calls，其实可以在不创造这个实例的情况下直接添加列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Total charge&#x27;</span>] = df[<span class="hljs-string">&#x27;Total day charge&#x27;</span>] + df[<span class="hljs-string">&#x27;Total eve charge&#x27;</span>] + \<br>    df[<span class="hljs-string">&#x27;Total night charge&#x27;</span>] + df[<span class="hljs-string">&#x27;Total intl charge&#x27;</span>]<br>df.head()<br><br></code></pre></td></tr></table></figure>

<p>使用 <code>drop()</code> 方法删除列和行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 移除先前创捷的列</span><br>df.drop([<span class="hljs-string">&#x27;Total charge&#x27;</span>, <span class="hljs-string">&#x27;Total calls&#x27;</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 删除行</span><br>df.drop([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]).head()<br></code></pre></td></tr></table></figure>

<p>对上述代码的部分解释：</p>
<ul>
<li>将相应的索引 <code>[&#39;Total charge&#39;, &#39;Total calls&#39;]</code> 和 <code>axis</code> 参数（1 表示删除列，0 表示删除行，默认值为 0）传给 <code>drop</code>。</li>
<li><code>inplace</code> 参数表示是否修改原始 DataFrame （False 表示不修改现有 DataFrame，返回一个新 DataFrame，True 表示修改当前 DataFrame）。</li>
</ul>
<h2 id="预测离网率"><a href="#预测离网率" class="headerlink" title="预测离网率"></a>预测离网率</h2><p>首先，通过上面介绍的 <code>crosstab()</code> 方法构建一个交叉表来查看 International plan 国际套餐 变量和 Churn 离网率 的相关性，同时使用 <code>countplot()</code> 方法构建计数直方图来可视化结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载模块，配置绘图</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>sns.countplot(x=<span class="hljs-string">&#x27;International plan&#x27;</span>, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df)<br></code></pre></td></tr></table></figure>

<p>上图表明，开通了国际套餐的用户的离网率要高很多，这是一个很有趣的观测结果。也许，国际电话高昂的话费让客户很不满意。</p>
<p>同理，查看 Customer service calls 客服呼叫 变量与 Chunrn 离网率 的相关性，并可视化结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.crosstab(df[<span class="hljs-string">&#x27;Churn&#x27;</span>], df[<span class="hljs-string">&#x27;Customer service calls&#x27;</span>], margins=<span class="hljs-literal">True</span>)<br><br>sns.countplot(x=<span class="hljs-string">&#x27;Customer service calls&#x27;</span>, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df)<br></code></pre></td></tr></table></figure>

<p>上图表明，在客服呼叫 4 次之后，客户的离网率显著下降。</p>
<p>为了更好的突出 Customer service call 客服呼叫 和 Churn 离网率 的关系，可以给 DataFrame 添加一个二元属性 Many_service_calls，即客户呼叫超过 3 次（Customer service calls &gt; 3）。看下它与离网率的相关性，并可视化结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Many_service_calls&#x27;</span>] = (df[<span class="hljs-string">&#x27;Customer service calls&#x27;</span>] &gt; <span class="hljs-number">3</span>).astype(<span class="hljs-string">&#x27;int&#x27;</span>)<br><br>pd.crosstab(df[<span class="hljs-string">&#x27;Many_service_calls&#x27;</span>], df[<span class="hljs-string">&#x27;Churn&#x27;</span>], margins=<span class="hljs-literal">True</span>)<br><br>sns.countplot(x=<span class="hljs-string">&#x27;Many_service_calls&#x27;</span>, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df)<br></code></pre></td></tr></table></figure>

<p>现在我们可以创建另一张交叉表，将 Churn 离网率 与 International plan 国际套餐 及新创建的 Many_service_calls 多次客服呼叫 关联起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.crosstab(df[<span class="hljs-string">&#x27;Many_service_calls&#x27;</span>] &amp; df[<span class="hljs-string">&#x27;International plan&#x27;</span>], df[<span class="hljs-string">&#x27;Churn&#x27;</span>])<br></code></pre></td></tr></table></figure>

<p>上表表明，在客服呼叫次数超过 3 次并且已办理 International Plan 国际套餐 的情况下，预测一名客户不忠诚的准确率（Accuracy）可以达到 85.8％，计算公式如下：</p>
<p>准确率（𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦）&#x3D;𝑇𝑃+𝑇𝑁𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁&#x3D;2841+192841+9+19+464×100准确率（<em>A<strong>c</strong>c<strong>u</strong>r<strong>a</strong>c**y</em>）&#x3D;<em>T**P</em>+<em>T**N</em>+<em>F**P</em>+<em>F<strong>N</strong>T**P</em>+<em>T**N</em>&#x3D;2841+9+19+4642841+19×100</p>
<p>其中，TP 表示将 True 预测为 True 的数量，TN 表示将 Flase 预测为 Flase 的数量，FP 表示将 Flase 预测为 True 的数量，FN 表示将 True 预测为 Flase 的数量。</p>
<p>复习一下本次实验的内容：</p>
<ul>
<li>样本中忠实客户的份额为 85.5%。这意味着最简单的预测「忠实客户」的模型有 85.5% 的概率猜对。也就是说，后续模型的准确率（Accuracy）不应该比这个数字少，并且很有希望显著高于这个数字。</li>
<li>基于一个简单的「（客服呼叫次数 &gt; 3） &amp; （国际套餐 &#x3D; True） &#x3D;&gt; Churn &#x3D; 1, else Churn &#x3D; 0」规则的预测模型，可以得到 85.8% 的准确率。以后我们将讨论决策树，看看如何仅仅基于输入数据自动找出类似的规则，而不需要我们手工设定。我们没有应用机器学习方法就得到了两个准确率（85.5% 和 85.8%），它们可作为后续其他模型的基线。如果经过大量的努力，我们仅将准确率提高了 0.5%，那么我们努力的方向可能出现了偏差，因为仅仅使用一个包含两个限制规则的简单模型就已提升了 0.3% 的准确率。</li>
<li>在训练复杂模型之前，建议预处理一下数据，绘制一些图表，做一些简单的假设。此外，在实际任务上应用机器学习时，通常从简单的方案开始，接着尝试更复杂的方案。</li>
</ul>
<h3 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h3><p>本次实验使用 Pandas 对数据进行了一定程度的分析和探索，交叉表、透视表等方法的运用将使你在数据探索过程中事半功倍。</p>
<p> <em>相关链接</em></p>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://pandas.pydata.org/pandas-docs/stable/index.html"> <em>Pandas 官方文档</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://pandas.pydata.org/pandas-docs/stable/10min.html"> <em>10 minutes to pandas</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf"> <em>Pandas cheatsheet PDF</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.scipy-lectures.org/index.html"> <em>scipy-lectures.org 教程</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/louplus/"> <em>了解蓝桥云课《楼+ 机器学习和数据挖掘课程》</em></a></li>
</ul>
<h2 id="Python-数据可视化分析"><a href="#Python-数据可视化分析" class="headerlink" title="Python 数据可视化分析"></a>Python 数据可视化分析</h2><hr>
<h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>在机器学习领域中，可视化是十分重要的。在开始一项新任务时，通过可视化手段探索数据能更好地帮助人们把握数据的要点。在分析模型表现和模型报告的结果时，可视化能使分析显得更加生动鲜明。有时候，为了理解复杂的模型，我们还可以将高维空间映射为视觉上更直观的二维或三维图形。</p>
<p>总而言之，可视化是一个相对快捷的从数据中挖掘信息的手段。本文将使用 Pandas、Matplotlib、seaborn 等流行的库，带你上手可视化。</p>
<h4 id="知识点-1"><a href="#知识点-1" class="headerlink" title="知识点"></a>知识点</h4><ul>
<li>单变量可视化的常用方法</li>
<li>多变量可视化的常用方法</li>
<li>t-SNE</li>
</ul>
<hr>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>首先使用 <code>import</code> 载入相关依赖。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>sns.<span class="hljs-built_in">set</span>()<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>在第一篇文章中，我们使用的是某电信运营商的客户离网数据集，本次实验仍旧使用这个数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.read_csv(<br>    <span class="hljs-string">&#x27;https://labfile.oss.aliyuncs.com/courses/1283/telecom_churn.csv&#x27;</span>)<br><br>df.head()<br></code></pre></td></tr></table></figure>

<p>最后一个数据列 Churn 离网率 是我们的目标特征，它是布尔变量，其中 True 表示公司最终丢失了此客户，False 表示客户被保留。稍后，将构建基于其他特征预测 Churn 特征的模型。</p>
<h3 id="单变量可视化"><a href="#单变量可视化" class="headerlink" title="单变量可视化"></a>单变量可视化</h3><p>单变量（univariate）分析一次只关注一个变量。当我们独立地分析一个特征时，通常最关心的是该特征值的分布情况。下面考虑不同统计类型的变量，以及相应的可视化工具。</p>
<h4 id="数量特征"><a href="#数量特征" class="headerlink" title="数量特征"></a>数量特征</h4><p>数量特征（quantitative feature）的值为有序数值。这些值可能是离散的，例如整数，也可能是连续的，例如实数。</p>
<h4 id="直方图和密度图"><a href="#直方图和密度图" class="headerlink" title="直方图和密度图"></a>直方图和密度图</h4><p>直方图依照相等的间隔将值分组为柱，它的形状可能包含了数据分布的一些信息，如高斯分布、指数分布等。当分布总体呈现规律性，但有个别异常值时，你可以通过直方图辨认出来。当你使用的机器学习方法预设了某一特定分布类型（通常是高斯分布）时，知道特征值的分布是非常重要的。</p>
<p>最简单的查看数值变量分布的方法是使用 DataFrame 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html"> <em><code>hist()</code></em></a> 方法绘制直方图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">features = [<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, <span class="hljs-string">&#x27;Total intl calls&#x27;</span>]<br>df[features].hist(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>

<p>上图表明，变量 Total day minutes 每日通话时长 呈高斯分布，而 Total intl calls 总国际呼叫数 显著右倾（它右侧的尾巴更长）。</p>
<p>密度图（density plots），也叫核密度图（kernel density estimate，KDE）是理解数值变量分布的另一个方法。它可以看成是直方图平滑（smoothed）的版本。相比直方图，它的主要优势是不依赖于柱的尺寸，更加清晰。</p>
<p>让我们为上面两个变量创建密度图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df[features].plot(kind=<span class="hljs-string">&#x27;density&#x27;</span>, subplots=<span class="hljs-literal">True</span>, layout=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>                  sharex=<span class="hljs-literal">False</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>), legend=<span class="hljs-literal">False</span>, title=features)<br></code></pre></td></tr></table></figure>

<p>当然，还可以使用 seaborn 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/generated/seaborn.distplot.html"> <em><code>distplot()</code></em></a> 方法观测数值变量的分布。例如，Total day minutes 每日通话时长 的分布。默认情况下，该方法将同时显示直方图和密度图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.distplot(df[<span class="hljs-string">&#x27;Total intl calls&#x27;</span>])<br></code></pre></td></tr></table></figure>

<p>上图中直方图的柱形高度已进行归一化处理，表示的是密度而不是样本数。</p>
<h4 id="箱型图"><a href="#箱型图" class="headerlink" title="箱型图"></a>箱型图</h4><p>箱形图的主要组成部分是箱子（box），须（whisker）和一些单独的数据点（离群值），分别简单介绍如下：</p>
<ul>
<li>箱子显示了分布的四分位距，它的长度由 25𝑡ℎ,（Q1，下四分位数）25<em>t**h</em>,（Q1，下四分位数） 和 75𝑡ℎ,（Q3，上四分位数）75<em>t**h</em>,（Q3，上四分位数） 决定，箱中的水平线表示中位数 （5050）。</li>
<li>须是从箱子处延伸出来的线，它们表示数据点的总体散布，具体而言，是位于区间 （Q1−1.5⋅IQR,Q3+1.5⋅IQR）（Q1−1.5⋅IQR,Q3+1.5⋅IQR）的数据点，其中 IQR&#x3D;Q3−Q1IQR&#x3D;Q3−Q1，也就是四分位距。</li>
<li>离群值是须之外的数据点，它们作为单独的数据点，沿着中轴绘制。</li>
</ul>
<p>使用 seaborn 的 <code>boxplot()</code> 方法绘制箱形图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.boxplot(x=<span class="hljs-string">&#x27;Total intl calls&#x27;</span>, data=df)<br></code></pre></td></tr></table></figure>

<p>上图表明，在该数据集中，大量的国际呼叫是相当少见的。</p>
<h4 id="提琴形图"><a href="#提琴形图" class="headerlink" title="提琴形图"></a>提琴形图</h4><p>我们最后考虑的分布图形是提琴形图（violin plot）。提琴形图和箱形图的区别是，提琴形图聚焦于平滑后的整体分布，而箱形图显示了单独样本的特定统计数据。</p>
<p>使用 <code>violinplot()</code> 方法绘制提琴形图。下图左侧是箱形图，右侧是提琴形图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">_, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">4</span>))<br>sns.boxplot(data=df[<span class="hljs-string">&#x27;Total intl calls&#x27;</span>], ax=axes[<span class="hljs-number">0</span>])<br>sns.violinplot(data=df[<span class="hljs-string">&#x27;Total intl calls&#x27;</span>], ax=axes[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>

<h4 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h4><p>除图形工具外，还可以使用 DataFrame 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html"> <em><code>describe()</code></em></a> 方法来获取分布的精确数值统计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[features].describe()<br></code></pre></td></tr></table></figure>

<p><code>describe()</code> 的输出基本上是自解释性的，25%，50% 和 75% 是相应的百分数。</p>
<h4 id="类别特征和二元特征"><a href="#类别特征和二元特征" class="headerlink" title="类别特征和二元特征"></a>类别特征和二元特征</h4><p>类别特征（categorical features take）反映了样本的某个定性属性，它具有固定数目的值，每个值将一个观测数据分配到相应的组，这些组称为类别（category）。如果类别变量的值具有顺序，称为有序（ordinal）类别变量。</p>
<p>二元（binary）特征是类别特征的特例，其可能值有 2 个。</p>
<h4 id="频率表"><a href="#频率表" class="headerlink" title="频率表"></a>频率表</h4><p>让我们查看一下目标变量 Churn 离网率 的分布情况。首先，使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"> <em><code>value_counts()</code></em></a> 方法得到一张频率表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;Churn&#x27;</span>].value_counts()<br></code></pre></td></tr></table></figure>

<p>上表显示，该数据集的 Churn 有 2850 个属于 False（Churn&#x3D;&#x3D;0），有 483 个属于 True（Churn&#x3D;&#x3D;1），数据集中忠实客户（Churn&#x3D;&#x3D;0）和不忠实客户（Churn&#x3D;&#x3D;1）的比例并不相等。我们将在以后的文章中看到，这种数据不平衡的情况会导致建立的分类模型存在一定的问题。在这种情况下，构建分类模型可能需要加重对「少数数据（在这里是 Churn&#x3D;&#x3D;1）分类错误」这一情况的惩罚。</p>
<h4 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h4><p>频率表的图形化表示是条形图。创建条形图最简单的方法是使用 seaborn 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/generated/seaborn.countplot.html"> <em><code>countplot()</code></em></a> 函数。让我们来画出两个分类变量的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">_, axes = plt.subplots(nrows=<span class="hljs-number">1</span>, ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">4</span>))<br><br>sns.countplot(x=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df, ax=axes[<span class="hljs-number">0</span>])<br>sns.countplot(x=<span class="hljs-string">&#x27;Customer service calls&#x27;</span>, data=df, ax=axes[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>

<p>条形图和直方图的区别如下：</p>
<ul>
<li>直方图适合查看数值变量的分布，而条形图用于查看类别特征。</li>
<li>直方图的 X 轴是数值；条形图的 X 轴可能是任何类型，如数字、字符串、布尔值。</li>
<li>直方图的 X 轴是一个笛卡尔坐标轴；条形图的顺序则没有事先定义。</li>
</ul>
<p>上左图清晰地表明了目标变量的失衡性。上右图则表明大部分客户最多打了 2-3 个客服电话就解决了他们的问题。不过，既然想要预测少数数据的分类（Churn&#x3D;&#x3D;1），我们可能对少数不满意的客户的表现更感兴趣。所以让我们尝试一下更有趣的可视化方法：多变量可视化，看能否对预测有所帮助。</p>
<h3 id="多变量可视化"><a href="#多变量可视化" class="headerlink" title="多变量可视化"></a>多变量可视化</h3><p>多变量（multivariate）图形可以在单张图像中查看两个以上变量的联系，和单变量图形一样，可视化的类型取决于将要分析的变量的类型。</p>
<p>先来看看数量变量之间的相互作用。</p>
<h4 id="相关矩阵"><a href="#相关矩阵" class="headerlink" title="相关矩阵"></a>相关矩阵</h4><p>相关矩阵可揭示数据集中的数值变量的相关性。这一信息很重要，因为有一些机器学习算法（比如，线性回归和逻辑回归）不能很好地处理高度相关的输入变量。</p>
<p>首先，我们使用 DataFrame 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html"> <em><code>corr()</code></em></a> 方法计算出每对特征间的相关性。接着，我们将所得的相关矩阵（correlation matrix）传给 seaborn 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/generated/seaborn.heatmap.html"> <em><code>heatmap()</code></em></a>方法，该方法根据提供的数值，渲染出一个基于色彩编码的矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 丢弃非数值变量</span><br>numerical = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(df.columns) -<br>                 <span class="hljs-built_in">set</span>([<span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;International plan&#x27;</span>, <span class="hljs-string">&#x27;Voice mail plan&#x27;</span>,<br>                      <span class="hljs-string">&#x27;Area code&#x27;</span>, <span class="hljs-string">&#x27;Churn&#x27;</span>, <span class="hljs-string">&#x27;Customer service calls&#x27;</span>]))<br><span class="hljs-comment"># 计算和绘图</span><br>corr_matrix = df[numerical].corr()<br>sns.heatmap(corr_matrix)<br></code></pre></td></tr></table></figure>

<p>上图中，Total day charge 日话费总额 是直接基于 Total day minutes 电话的分钟数 计算得到，它被称为因变量。除了 Total day charge 外，还有 3 个因变量：Total eve charge，Total night charge，Total intl charge。这 4 个因变量并不贡献任何额外信息，我们直接去除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">numerical = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(numerical) - <span class="hljs-built_in">set</span>([<span class="hljs-string">&#x27;Total day charge&#x27;</span>, <span class="hljs-string">&#x27;Total eve charge&#x27;</span>, <br>                                       <span class="hljs-string">&#x27;Total night charge&#x27;</span>, <span class="hljs-string">&#x27;Total intl charge&#x27;</span>]))<br></code></pre></td></tr></table></figure>

<h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><p>散点图（scatter plot）将两个数值变量的值显示为二维空间中的笛卡尔坐标（Cartesian coordinate）。通过 matplotlib 库的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html"> <em><code>scatter()</code></em></a> 方法可以绘制散点图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(df[<span class="hljs-string">&#x27;Total day minutes&#x27;</span>], df[<span class="hljs-string">&#x27;Total night minutes&#x27;</span>])<br></code></pre></td></tr></table></figure>

<p>我们得到了两个正态分布变量的散点图，看起来这两个变量并不相关，因为上图的形状和轴是对齐的。</p>
<p>seaborn 库的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/generated/seaborn.jointplot.html"> <em><code>jointplot()</code></em></a> 方法在绘制散点图的同时会绘制两张直方图，某些情形下它们可能会更有用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.jointplot(x=<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, y=<span class="hljs-string">&#x27;Total night minutes&#x27;</span>,<br>              data=df, kind=<span class="hljs-string">&#x27;scatter&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><code>jointplot()</code> 方法还可以绘制平滑过的散点直方图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.jointplot(<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, <span class="hljs-string">&#x27;Total night minutes&#x27;</span>, data=df,<br>              kind=<span class="hljs-string">&quot;kde&quot;</span>, color=<span class="hljs-string">&quot;g&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>上图基本上就是之前讨论过的核密度图的双变量版本。</p>
<h4 id="散点图矩阵"><a href="#散点图矩阵" class="headerlink" title="散点图矩阵"></a>散点图矩阵</h4><p>在某些情形下，我们可能想要绘制如下所示的散点图矩阵（scatterplot matrix）。它的对角线包含变量的分布，并且每对变量的散点图填充了矩阵的其余部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">%config InlineBackend.figure_format = <span class="hljs-string">&#x27;png&#x27;</span><br>sns.pairplot(df[numerical])<br></code></pre></td></tr></table></figure>

<h4 id="数量和类别"><a href="#数量和类别" class="headerlink" title="数量和类别"></a>数量和类别</h4><p>为了让图形更有趣一点，可以尝试从数值和类别特征的相互作用中得到预测 Churn 的新信息，更具体地，让我们看看输入变量和目标变量 Churn 的关系。使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/generated/seaborn.lmplot.html"> <em><code>lmplot()</code></em></a> 方法的 hue 参数来指定感兴趣的类别特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.lmplot(<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, <span class="hljs-string">&#x27;Total night minutes&#x27;</span>,<br>           data=df, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, fit_reg=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>

<p>看起来不忠实客户偏向右上角，也就是倾向于在白天和夜间打更多电话的客户。当然，这不是非常明显，我们也不会基于这一图形下任何确定性的结论。</p>
<p>现在，创建箱形图，以可视化忠实客户（Churn&#x3D;0）和离网客户（Churn&#x3D;1）这两个互斥分组中数值变量分布的统计数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 有时我们可以将有序变量作为数值变量分析</span><br>numerical.append(<span class="hljs-string">&#x27;Customer service calls&#x27;</span>)<br><br>fig, axes = plt.subplots(nrows=<span class="hljs-number">3</span>, ncols=<span class="hljs-number">4</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))<br><span class="hljs-keyword">for</span> idx, feat <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(numerical):<br>    ax = axes[<span class="hljs-built_in">int</span>(idx / <span class="hljs-number">4</span>), idx % <span class="hljs-number">4</span>]<br>    sns.boxplot(x=<span class="hljs-string">&#x27;Churn&#x27;</span>, y=feat, data=df, ax=ax)<br>    ax.set_xlabel(<span class="hljs-string">&#x27;&#x27;</span>)<br>    ax.set_ylabel(feat)<br>fig.tight_layout()<br></code></pre></td></tr></table></figure>

<p>上面的图表表明，两组之间分歧最大的分布是这三个变量：Total day minutes 日通话分钟数、Customer service calls 客服呼叫数、Number vmail messages 语音邮件数。在后续的课程中，我们将学习如何使用随机森林（Random Forest）或梯度提升（Gradient Boosting）来判定特征对分类的重要性，届时可以清晰地看到，前两个特征对于离网预测模型而言确实非常重要。</p>
<p>创建箱型图和提琴形图，查看忠实客户和不忠实客户的日通话分钟数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">_, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br><br>sns.boxplot(x=<span class="hljs-string">&#x27;Churn&#x27;</span>, y=<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, data=df, ax=axes[<span class="hljs-number">0</span>])<br>sns.violinplot(x=<span class="hljs-string">&#x27;Churn&#x27;</span>, y=<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, data=df, ax=axes[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>

<p>上图表明，不忠实客户倾向于打更多的电话。</p>
<p>我们还可以发现一个有趣的信息：平均而言，离网客户是通讯服务更活跃的用户。或许是他们对话费不满意，所以预防离网的一个可能措施是降低通话费。当然，公司需要进行额外的经济分析，以查明这样做是否真的有利。</p>
<p>当想要一次性分析两个类别维度下的数量变量时，可以用 seaborn 库的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/generated/seaborn.factorplot.html"> <em><code>catplot()</code></em></a> 函数。例如，在同一图形中可视化 Total day minutes 日通话分钟数 和两个类别变量（Churn 和 Customer service calls）的相互作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.catplot(x=<span class="hljs-string">&#x27;Churn&#x27;</span>, y=<span class="hljs-string">&#x27;Total day minutes&#x27;</span>, col=<span class="hljs-string">&#x27;Customer service calls&#x27;</span>,<br>            data=df[df[<span class="hljs-string">&#x27;Customer service calls&#x27;</span>] &lt; <span class="hljs-number">8</span>], kind=<span class="hljs-string">&quot;box&quot;</span>,<br>            col_wrap=<span class="hljs-number">4</span>, height=<span class="hljs-number">3</span>, aspect=<span class="hljs-number">.8</span>)<br></code></pre></td></tr></table></figure>

<p>上图表明，从第 4 次客服呼叫开始，Total day minutes 日通话分钟数 可能不再是客户离网（Churn&#x3D;&#x3D;1）的主要因素。也许，除了我们之前猜测的话费原因，还有其他问题导致客户对服务不满意，这可能会导致日通话分钟数更少。</p>
<h4 id="类别与类别"><a href="#类别与类别" class="headerlink" title="类别与类别"></a>类别与类别</h4><p>正如之前提到的，变量 Customer service calls 客服呼叫数 的重复值很多，因此，既可以看成数值变量，也可以看成有序类别变量。之前已通过计数图（count plot）查看过它的分布了，现在我们感兴趣的是这一有序特征和目标变量 Churn 离网率 之间的关系。</p>
<p>使用 <code>countplot()</code> 方法查看客服呼叫数的分布，这次传入 <code>hue=Churn</code> 参数，以便在图形中加入类别维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.countplot(x=<span class="hljs-string">&#x27;Customer service calls&#x27;</span>, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df)<br></code></pre></td></tr></table></figure>

<p>上图表明，呼叫客服达到 4 次以上后，离网率显著增加了。</p>
<p>使用 <code>countplot()</code> 方法查看 Churn 离网率 和二元特征 International plan 国际套餐、Voice mail plan 语音邮件套餐 的关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">_, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br><br>sns.countplot(x=<span class="hljs-string">&#x27;International plan&#x27;</span>, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df, ax=axes[<span class="hljs-number">0</span>])<br>sns.countplot(x=<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>, hue=<span class="hljs-string">&#x27;Churn&#x27;</span>, data=df, ax=axes[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>

<p>上图表明，开通国际套餐后，离网率会高很多，即 International plan 是否开通国际套餐 是一个重要的特征。我们在 Vocie mail plan 语音邮件套餐 特征上没有观察到类似的效果。</p>
<h4 id="交叉表"><a href="#交叉表" class="headerlink" title="交叉表"></a>交叉表</h4><p>除了使用图形进行类别分析之外，还可以使用统计学的传统工具：交叉表（cross tabulation），即使用表格形式表示多个类别变量的频率分布。通过它可以查看某一列或某一行以了解某个变量在另一变量的作用下的分布情况。</p>
<p>通过交叉表查看 Churn 离网率 和类别变量 State 州 的关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.crosstab(df[<span class="hljs-string">&#x27;State&#x27;</span>], df[<span class="hljs-string">&#x27;Churn&#x27;</span>]).T<br></code></pre></td></tr></table></figure>

<p>上表显示，State 州 有 51 个不同的值，并且每个州只有 3 到 17 个客户抛弃了运营商。通过 <code>groupby()</code> 方法计算每个州的离网率，由高到低排列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">df.groupby([<span class="hljs-string">&#x27;State&#x27;</span>])[<span class="hljs-string">&#x27;Churn&#x27;</span>].agg(<br>    [np.mean]).sort_values(by=<span class="hljs-string">&#x27;mean&#x27;</span>, ascending=<span class="hljs-literal">False</span>).T<br></code></pre></td></tr></table></figure>

<p>上表显示，新泽西和加利福尼亚的离网率超过了 25%，夏威夷和阿拉斯加的离网率则不到 6%。然而，这些结论是基于极少的样本得出的，可能仅适用于这一特定数据集，不太具有泛用性。</p>
<h3 id="全局数据集可视化"><a href="#全局数据集可视化" class="headerlink" title="全局数据集可视化"></a>全局数据集可视化</h3><p>上面我们一直在研究数据集的不同方面（facet），通过猜测有趣的特征并一次选择少量特征进行可视化。如果我们想一次性显示所有特征并仍然能够解释生成的可视化，该怎么办？</p>
<h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><p>大多数现实世界的数据集有很多特征，每一个特征都可以被看成数据空间的一个维度。因此，我们经常需要处理高维数据集，然而可视化整个高维数据集相当难。为了从整体上查看一个数据集，需要在不损失很多数据信息的前提下，降低用于可视化的维度。这一任务被称为降维（dimensionality reduction）。降维是一个无监督学习（unsupervised learning）问题，因为它需要在不借助任何监督输入（如标签）的前提下，从数据自身得到新的低维特征。</p>
<p>主成分分析（Principal Component Analysis, PCA）是一个著名的降维方法，我们会在之后的课程中讨论它。但主成分分析的局限性在于，它是线性（linear）算法，这意味着对数据有某些特定的限制。</p>
<p>与线性方法相对的，有许多非线性方法，统称流形学习（Manifold Learning）。著名的流形学习方法之一是 t-SNE。</p>
<h4 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h4><p>它的基本思路很简单：为高维特征空间在二维平面（或三维平面）上寻找一个投影，使得在原本的 n 维空间中相距很远的数据点在二维平面上同样相距较远，而原本相近的点在平面上仍然相近。</p>
<p>该数据库创建一个 t-SNE 表示，首先加载依赖。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br></code></pre></td></tr></table></figure>

<p>去除 State 州 和 Churn 离网率 变量，然后用 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"> <em><code>pandas.Series.map()</code></em></a> 方法将二元特征的「Yes」&#x2F;「No」转换成数值。 :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X = df.drop([<span class="hljs-string">&#x27;Churn&#x27;</span>, <span class="hljs-string">&#x27;State&#x27;</span>], axis=<span class="hljs-number">1</span>)<br>X[<span class="hljs-string">&#x27;International plan&#x27;</span>] = X[<span class="hljs-string">&#x27;International plan&#x27;</span>].<span class="hljs-built_in">map</span>(&#123;<span class="hljs-string">&#x27;Yes&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;No&#x27;</span>: <span class="hljs-number">0</span>&#125;)<br>X[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>] = X[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>].<span class="hljs-built_in">map</span>(&#123;<span class="hljs-string">&#x27;Yes&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;No&#x27;</span>: <span class="hljs-number">0</span>&#125;)<br></code></pre></td></tr></table></figure>

<p>使用 <code>StandardScaler()</code> 方法来完成归一化数据，即从每个变量中减去均值，然后除以标准差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">scaler = StandardScaler()<br>X_scaled = scaler.fit_transform(X)<br></code></pre></td></tr></table></figure>

<p>现在可以构建 t-SNE 表示了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tsne = TSNE(random_state=<span class="hljs-number">17</span>)<br>tsne_repr = tsne.fit_transform(X_scaled)<br></code></pre></td></tr></table></figure>

<p>然后以图形的方式可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(tsne_repr[:, <span class="hljs-number">0</span>], tsne_repr[:, <span class="hljs-number">1</span>], alpha=<span class="hljs-number">.5</span>)<br></code></pre></td></tr></table></figure>

<p>根据离网情况给 t-SNE 表示加上色彩（蓝色表示忠实用户，黄色表示不忠实用户），形成离网情况散点图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(tsne_repr[:, <span class="hljs-number">0</span>], tsne_repr[:, <span class="hljs-number">1</span>],<br>            c=df[<span class="hljs-string">&#x27;Churn&#x27;</span>].<span class="hljs-built_in">map</span>(&#123;<span class="hljs-literal">False</span>: <span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-literal">True</span>: <span class="hljs-string">&#x27;orange&#x27;</span>&#125;), alpha=<span class="hljs-number">.5</span>)<br></code></pre></td></tr></table></figure>

<p>可以看到，离网客户集中在低维特征空间的一小部分区域。为了更好地理解这一图像，可以使用剩下的两个二元特征，即 International plan 国际套餐 和 Voice mail plan 语音邮件套餐 给图像着色，蓝色代表二元特征的值为 Yes，黄色代表二元特征的值为 No。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">_, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">5</span>))<br><br><span class="hljs-keyword">for</span> i, name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>([<span class="hljs-string">&#x27;International plan&#x27;</span>, <span class="hljs-string">&#x27;Voice mail plan&#x27;</span>]):<br>    axes[i].scatter(tsne_repr[:, <span class="hljs-number">0</span>], tsne_repr[:, <span class="hljs-number">1</span>],<br>                    c=df[name].<span class="hljs-built_in">map</span>(&#123;<span class="hljs-string">&#x27;Yes&#x27;</span>: <span class="hljs-string">&#x27;orange&#x27;</span>, <span class="hljs-string">&#x27;No&#x27;</span>: <span class="hljs-string">&#x27;blue&#x27;</span>&#125;), alpha=<span class="hljs-number">.5</span>)<br>    axes[i].set_title(name)<br></code></pre></td></tr></table></figure>

<p>通过上面 3 张图，我们就可以更直观的分析客户的离网原因了。</p>
<p>最后，了解下 t-SNE 的缺陷。</p>
<ul>
<li>计算复杂度高。如果你有大量样本，你应该使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/DmitryUlyanov/Multicore-TSNE"> <em>Multicore-TSNE</em></a>。</li>
<li>随机数种子的不同会导致图形大不相同，这给解释带来了困难。通常而言，你不应该基于这些图像做出任何决定性的结论，因为它可能和单纯的猜测差不多。当然，t-SNE 图像中的某些发现可能会启发一个想法，这个想法可以通过更全面深入的研究得到确认。</li>
</ul>
<h3 id="实验总结-1"><a href="#实验总结-1" class="headerlink" title="实验总结"></a>实验总结</h3><p>本章节首先介绍了 Pandas、Matplotlib 和 seaborn 库的一些常用可视化方法，并对客户离网数据集进行了可视化分析和 t-SNE 降维。可视化是一个相对快捷的从数据中挖掘信息的手段，因此，学习这一技术并将其纳入你的日常机器学习工具箱，是很有必要的。</p>
<p> <em>相关链接</em></p>
<ul>
<li>本文所用库的官方文档： <a target="_blank" rel="external nofollow noopener noreferrer" href="https://matplotlib.org/contents.html"> <em>Matplotlib</em></a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://seaborn.pydata.org/introduction.html"> <em>seaborn</em></a> 和 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://pandas.pydata.org/pandas-docs/stable/"> <em>Pandas</em></a>.</li>
<li>使用 seaborn 绘图的示例 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://seaborn.pydata.org/examples/index.html"> <em>Gallery</em></a>。</li>
<li>scikit-learn 的流形学习文档 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://scikit-learn.org/stable/modules/manifold.html"> <em>Documentation</em></a>。</li>
<li>高效的 t-SNE 实现 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/DmitryUlyanov/Multicore-TSNE"> <em>Multicore-TSNE</em></a>。</li>
<li>如何有效使用 t-SNE <a target="_blank" rel="external nofollow noopener noreferrer" href="https://distill.pub/2016/misread-tsne/"> <em>Distill.pub</em></a>。</li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/louplus/"> <em>了解蓝桥云课《楼+ 机器学习和数据挖掘课程》</em></a></li>
</ul>
<h2 id="决策树和-K-近邻分类"><a href="#决策树和-K-近邻分类" class="headerlink" title="决策树和 K 近邻分类"></a>决策树和 K 近邻分类</h2><hr>
<h4 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h4><p>本次实验分别运用决策树和最近邻方法在分类任务上构建模型，并通过交叉验证对模型进行调优。</p>
<h4 id="知识点-2"><a href="#知识点-2" class="headerlink" title="知识点"></a>知识点</h4><ul>
<li>决策树</li>
<li>最近邻方法</li>
<li>交叉验证</li>
</ul>
<hr>
<h3 id="机器学习介绍"><a href="#机器学习介绍" class="headerlink" title="机器学习介绍"></a>机器学习介绍</h3><p>在深入本次实验之前，首先了解一下什么是机器学习， Machine Learning（T. Mitchell 著，1997 年出版）一书中给出了机器学习经典、通用的定义：</p>
<blockquote>
<p>假设用 P 来评估计算机程序在某任务类 T 上的性能，若一个程序利用经验 E 在任务 T 上获得了性能改善，则我们就说关于 T 和 P, 该程序对 E 进行了学习。</p>
</blockquote>
<p>在不同的问题设定下，T、P、E 可能指完全不同的东西。机器学习中一些流行的任务 T 包括：</p>
<ul>
<li>分类：基于特征将实例分为某一类。</li>
<li>回归：基于实例的其他特征预测该实例的数值型目标特征。</li>
<li>聚类：基于实例的特征实现实例的分组，从而让组内成员比组间成员更为相似。</li>
<li>异常检测：寻找与其他样本或组内实例有很大区别的实例。</li>
<li>其他更多任务</li>
</ul>
<p>关于经验 E，《Deep Learning》（Ian Goodfellow、Yoshua Bengio、Aaron Courville 著，2016 年出版）的「Machine Learning basics」一章提供了一份很好的综述：</p>
<blockquote>
<p>经验 E 指的是数据（没有数据我们什么也干不了）。根据训练方式，机器学习算法可以分为监督（supervised）和无监督（unsupervised）两类。无监督学习需要训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。而监督学习的数据集除了含有很多特征外，它的每个样本都要有一个标签（label）或目标（target）。</p>
</blockquote>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>分类和回归属于监督学习问题。例如，作为信贷机构，我们可能希望根据客户累积的数据预测贷款违约情况。在这里，经验 E 是已有的训练数据，即实例（客户）的集合，一组特征（例如年龄、薪水、贷款类型、以往违约记录等），一个目标变量（他们是否会违约）。由于需要预测的目标变量是「他们是否会违约」，所以这是一个二元分类问题。如果你转而预测贷款会超期多久，那么需要预测的目标变量变成了一个连续值（时间），这就成为一个回归问题了。</p>
<p>最后，关于算法表现的评估度量 P。不同问题和算法的度量不同，当学习新算法时，我们将讨论这一点。就目前而言，本次实验将使用分类算法中的一个简单度量标准，即准确率（Accuracy）。</p>
<p>下面看看分类和回归这两个监督学习问题。</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树是分类与回归问题中常用的方法之一。其实不仅是机器学习领域，在每天的日常决策中，我们都在使用决策树。流程图实际上就是决策树的可视化表示，例如，下面是俄罗斯国立高等经济研究大学（Higher School of Economics）提供的关于「如何在学院网站上发表论文」的流程图：</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175317777.png" srcset="/img/loading.gif" lazyload class title="img">

<p>用机器学习的术语来说，可以把它看成一个简单的分类器，根据内容（书、小册子、论文）、新闻类型、原发表物类型（科学期刊、通讯）等来确定合适的发表类型（书、文章、书的章节、预印本、Higher School of Economics and the Media 稿件）。</p>
<p>决策树常常是专家经验的概括，是一种分享特定过程知识的方式。例如，在引入可扩展机器学习算法之前，银行业的信用评分任务是由专家解决的，能否放贷是基于一些直观（或经验）的规则，这些规则就可以表示为决策树的形式，如下图所示：</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175318456.png" srcset="/img/loading.gif" lazyload class title="img">

<p>作为机器学习算法的决策树基本上和上图差不多，它合并一连串逻辑规则，使之成为一个树形的数据结构，这些规则的形式为「特征 a 的值小于 x，特征 b 的值小于 y … &#x3D;&gt; 类别 1」。</p>
<p>下面，我们基于「年龄」、「房产」、「收入」、「教育」特征使用决策树解决一个二元分类问题，即「是否允许贷款」。</p>
<h3 id="如何构建决策树"><a href="#如何构建决策树" class="headerlink" title="如何构建决策树"></a>如何构建决策树</h3><p>年龄、房产、收入、教育，这么多的特征首先应该关注哪个呢？</p>
<p>为了回答上述问题，先看一个简单的游戏，即「20个问题」游戏，这个游戏是这样玩的：A 心里想着一个名人，B 问 A 20 个问题，A 只能回答「是」或「否」，20 个问题之后 B 要猜出 A 心里想的那个名人是谁。首先问一个可以最大程度压缩剩余选项数目的问题会使 B 占据极大优势，例如询问「是不是安吉丽娜·朱莉？」，最多剔除一个选项，而询问「这个名人是女人吗？」将消除大约一半的选项。就是说，「性别」特征相比「安吉丽娜·朱莉」、「西班牙人」、「喜欢足球」等其他特征更能区分名人数据集。这背后的道理与熵有关，下面介绍熵的概念。</p>
<h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><p>熵是一个在物理、信息论和其他领域中广泛应用的重要概念，可以衡量获得的信息量。对于具有 N 种可能状态的系统而言，熵的定义如下：</p>
<p>$$S &#x3D; -\sum_{i&#x3D;1}^{N}p_i \log_2{p_i},$$</p>
<p>其中，𝑝𝑖<em>p**i</em> 是系统位于第 i 个状态的概率。熵可以描述为系统的混沌程度，熵越高，系统的有序性越差，反之亦然。熵将帮助我们高效的分割数据，类似帮助我们找出在「20个问题」游戏中先问什么问题较好。</p>
<h4 id="玩具示例"><a href="#玩具示例" class="headerlink" title="玩具示例"></a>玩具示例</h4><p>为了解释熵是如何有利于构建决策树模型的，让我们来看一个玩具示例，在这个示例中将基于球的位置预测它的颜色。</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175319246.png" srcset="/img/loading.gif" lazyload class title="img">

<p>这里有 9 个蓝球和 11 个黄球。如果随机选择一个球，这个球是蓝球的概率 $ 𝑝1&#x3D; \frac{9}{20}$ ，是黄球的概率 $ 𝑝2 &#x3D; \frac{11}{20}$，这意味着熵<br>$ 𝑆0&#x3D;− \frac{9}{20}{\log_{2}{\frac{9}{20}}}⁡ − \frac{11}{20}\log_{⁡2}{\frac{11}{20}} ≈ 1 $</p>
<p>这个值本身可能无法告诉我们很多信息。</p>
<p>将球分为「位置小于等于 12、位置大于 12」这两组，如下图所示。</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175320313.png" srcset="/img/loading.gif" lazyload class title="img">

<p>那么分组后，熵的变化如何？左边一组有 13 个球， 8 蓝 5 黄。这一组的熵 </p>
<p>$𝑆1&#x3D; −\frac{5}{13}{\log_{2}{\frac{5}{13}}}⁡ − \frac{8}{13}\log_{⁡2}{\frac{8}{13}} ≈ 0.96$</p>
<p>右边一组有 7 个球， 1 蓝 6 黄。右边这组的熵</p>
<p>$𝑆1&#x3D; −\frac{1}{7}{\log_{2}{\frac{1}{7}}}⁡ − \frac{6}{7}\log_{⁡2}{\frac{6}{7}} ≈ 0.6$</p>
<p>可见，两组的熵都下降了，且右边这组降得更多。由于熵实际上是系统混沌（或不确定）的程度，熵的下降被称为信息增益。数学上，基于变量 Q（在这个例子中是变量「x ≤ 12」）所作的分割，得到的信息增益（IG）定义为：</p>
<p>$$IG(Q) &#x3D; S_O - \sum_{i&#x3D;1}^{q}\frac{N_i}{N}S_i,$$</p>
<p>其中，𝑞<em>q</em> 是分割的组数，𝑁𝑖<em>N**i</em> 是变量 Q 等于第 i 项时的样本数目。在玩具示例中，有 2 个组（𝑞&#x3D;2<em>q</em>&#x3D;2），一组有 13 个元素（𝑁1&#x3D;13<em>N</em>1&#x3D;13），另一组有 7 个（𝑁2&#x3D;7<em>N</em>2&#x3D;7）。因此，信息增益为：</p>
<p>$$ IG(x \leq 12) &#x3D; S_0 - \frac{13}{20}S_1 - \frac{7}{20}S_2 \approx 0.16.$$</p>
<p>结果表明，根据「坐标小于或等于12」将球分为两组带来了一个更有序的系统。让我们继续分组，直到每组中的球颜色都一样。</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175320995.png" srcset="/img/loading.gif" lazyload class title="img">

<p>上图可见，右边那组只需根据「坐标小于或等于 18」再分割一次即可。而左边那组还需要三次分割。注意，若组内所有球的颜色都一样，那么这个组的熵为 0（$\log_2{1} &#x3D; 0$）。</p>
<p>通过这个例子，我们成功构建了一个基于球的位置预测球颜色的决策树。但倘若我们再向里面增加一个球，这个决策树就可能无法很好地工作，因为它完全拟合了训练集（初始的 20 球）。如果希望提升它的泛用性，那么一棵具有更少分支（「问题」）的决策树将有更好的效果。</p>
<h3 id="决策树构建算法"><a href="#决策树构建算法" class="headerlink" title="决策树构建算法"></a>决策树构建算法</h3><p>在之前的例子中构建的决策树是最优的：它只需提 5 个「问题」（基于变量 Q），就完全拟合了训练集。其他分割条件会使得到的树更深，即需要更多「问题」才能获得答案。</p>
<p>构建决策树的流行算法（如 ID3 或 C4.5）的核心，是贪婪最大化信息增益：在每一步，算法都会选择能在分割后给出最大信息增益的变量。接着递归重复这一流程，直到熵为零（或者，为了避免过拟合，直到熵为某个较小的值）。不同的算法使用不同的推断，通过「提前停止」或「截断」以避免构建出过拟合的树。</p>
<h3 id="分类问题中其他的分割质量标准"><a href="#分类问题中其他的分割质量标准" class="headerlink" title="分类问题中其他的分割质量标准"></a>分类问题中其他的分割质量标准</h3><p>上面我们讨论了熵是如何衡量树的分区的，但还有其他指标来衡量分割的好坏：</p>
<ul>
<li>基尼不确定性（Gini uncertainty）：$G &#x3D; 1 - \sum\limits_k (p_k)^2$</li>
<li>错分率（Misclassification error）：$E &#x3D; 1 - \max\limits_k p_k$</li>
</ul>
<p>实践中几乎从不使用错分率，而基尼不确定性和信息增益的效果差不多。</p>
<p>二元分类问题的熵和基尼不确定性为：</p>
<p>$$ S &#x3D; -p_+ \log_2{p_+} -p_- \log_2{p_-} &#x3D; -p_+ \log_2{p_+} -(1 - p_{+}) \log_2{(1 - p_{+})}$$</p>
<p>$$ G &#x3D; 1 - p_+^2 - p_-^2 &#x3D; 1 - p_+^2 - (1 - p_+)^2 &#x3D; 2p_+(1-p_+)$$</p>
<p>其中 $p_+$ 是对象具有标签 + 的概率。</p>
<p>以 $p_+$ 为坐标，绘制上面两个函数的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>sns.<span class="hljs-built_in">set</span>()<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br>plt.figure(figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">4</span>))<br>xx = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)<br>plt.plot(xx, [<span class="hljs-number">2</span> * x * (<span class="hljs-number">1</span>-x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xx], label=<span class="hljs-string">&#x27;gini&#x27;</span>)<br>plt.plot(xx, [<span class="hljs-number">4</span> * x * (<span class="hljs-number">1</span>-x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xx], label=<span class="hljs-string">&#x27;2*gini&#x27;</span>)<br>plt.plot(xx, [-x * np.log2(x) - (<span class="hljs-number">1</span>-x) * np.log2(<span class="hljs-number">1</span> - x)<br>              <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xx], label=<span class="hljs-string">&#x27;entropy&#x27;</span>)<br>plt.plot(xx, [<span class="hljs-number">1</span> - <span class="hljs-built_in">max</span>(x, <span class="hljs-number">1</span>-x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xx], label=<span class="hljs-string">&#x27;missclass&#x27;</span>)<br>plt.plot(xx, [<span class="hljs-number">2</span> - <span class="hljs-number">2</span> * <span class="hljs-built_in">max</span>(x, <span class="hljs-number">1</span>-x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xx], label=<span class="hljs-string">&#x27;2*missclass&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;p+&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;criterion&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Criteria of quality as a function of p+ (binary classification)&#x27;</span>)<br>plt.legend()<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175321442.png" srcset="/img/loading.gif" lazyload class title="image-20240618114618952">

<p>上图可见，熵的图像和两倍的基尼不确定性图像非常接近。因此，在实践中，这两个指标的效果基本上是一样的。</p>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p>下面用一棵决策树拟合一些合成数据。这些合成数据属于两个不同的类别，这两个类别的均值不同，但都呈现正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第一类</span><br>np.random.seed(<span class="hljs-number">17</span>)<br>train_data = np.random.normal(size=(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>train_labels = np.zeros(<span class="hljs-number">100</span>)<br><br><span class="hljs-comment"># 第二类</span><br>train_data = np.r_[train_data, np.random.normal(size=(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>), loc=<span class="hljs-number">2</span>)]<br>train_labels = np.r_[train_labels, np.ones(<span class="hljs-number">100</span>)]<br></code></pre></td></tr></table></figure>

<p>下面绘制数据。通俗地讲，这种情况下的分类问题就是构造一个「边界」，能够较好的分开两个类别（红点和黄点）。这个「边界」若是一条直线的话可能太过简单，若是沿着每个红点画出的蛇形曲线又太过复杂（这将导致其在新数据上的表现很差）。从直觉上说，某种平滑的边界，在新数据上的效果会比较好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>plt.scatter(train_data[:, <span class="hljs-number">0</span>], train_data[:, <span class="hljs-number">1</span>], c=train_labels, s=<span class="hljs-number">100</span>,<br>            cmap=<span class="hljs-string">&#x27;autumn&#x27;</span>, edgecolors=<span class="hljs-string">&#x27;black&#x27;</span>, linewidth=<span class="hljs-number">1.5</span>)<br>plt.plot(<span class="hljs-built_in">range</span>(-<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>

<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100620528.png" srcset="/img/loading.gif" lazyload alt="image-20240618114653486"></p>
<p>下面训练一棵 sklearn 决策树，区分这两类数据点。最后可视化所得的边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><span class="hljs-comment"># 编写一个辅助函数，返回之后的可视化网格</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_grid</span>(<span class="hljs-params">data</span>):<br>    x_min, x_max = data[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, data[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>    y_min, y_max = data[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, data[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.01</span>), np.arange(y_min, y_max, <span class="hljs-number">0.01</span>))<br><br><br><span class="hljs-comment">#  max_depth参数限制决策树的深度</span><br>clf_tree = DecisionTreeClassifier(criterion=<span class="hljs-string">&#x27;entropy&#x27;</span>, max_depth=<span class="hljs-number">3</span>,<br>                                  random_state=<span class="hljs-number">17</span>)<br><span class="hljs-comment"># 训练决策树</span><br>clf_tree.fit(train_data, train_labels)<br><span class="hljs-comment"># 可视化</span><br>xx, yy = get_grid(train_data)<br>predicted = clf_tree.predict(np.c_[xx.ravel(),<br>                                   yy.ravel()]).reshape(xx.shape)<br>plt.pcolormesh(xx, yy, predicted, cmap=<span class="hljs-string">&#x27;autumn&#x27;</span>)<br>plt.scatter(train_data[:, <span class="hljs-number">0</span>], train_data[:, <span class="hljs-number">1</span>], c=train_labels, s=<span class="hljs-number">100</span>,<br>            cmap=<span class="hljs-string">&#x27;autumn&#x27;</span>, edgecolors=<span class="hljs-string">&#x27;black&#x27;</span>, linewidth=<span class="hljs-number">1.5</span>)<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175323490.png" srcset="/img/loading.gif" lazyload class title="image-20240618115638248">

<p>通过 pydotplus 和 export_graphviz 库我们可以方便的看到决策树本身是怎样的。使用 <code>StringIO()</code> 函数开辟一个缓存空间保存决策树，通过 <code>export_graphviz()</code> 函数以 DOT 格式导出决策树的 GraphViz 表示，然后将其写入 out_file 中。使用 <code>graph_from_dot_data()</code> 函数读入数据并通过 <code>Image()</code> 函数显示决策树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">!pip install pydotplus  <span class="hljs-comment"># 安装必要模块</span><br><br><span class="hljs-keyword">from</span> ipywidgets <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> StringIO<br><span class="hljs-keyword">import</span> pydotplus<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> export_graphviz<br><br>dot_data = StringIO()<br>export_graphviz(clf_tree, feature_names=[<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>],<br>                out_file=dot_data, filled=<span class="hljs-literal">True</span>)<br>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())<br>Image(value=graph.create_png())<br></code></pre></td></tr></table></figure>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175324783.png" srcset="/img/loading.gif" lazyload class title="image-20240618120100425">)

<p>上图表明，在最深的一层，树将空间「切割」为8个矩形，也就是说，树有8个叶节点。在每个矩形之中，树将根据数量较多的对象的标签做出预测。</p>
<h3 id="我们如何「读懂」这颗决策树？"><a href="#我们如何「读懂」这颗决策树？" class="headerlink" title="我们如何「读懂」这颗决策树？"></a>我们如何「读懂」这颗决策树？</h3><p>上个示例中，总共有 200 个合成数据（样本），每个分类各有 100 个合成数据。初始状态的熵是最大的，即 𝑆&#x3D;1<em>S</em>&#x3D;1。接着，通过比较 𝑥2<em>x</em>2 与 1.211 的大小进行第一次分割，将样本分成两组（你可以在上图中找到这一部分边界）。基于这一次分割，左右两组的熵都下降了。这一过程持续进行，直到树的深度达到 3。在上图中，属于第一类的样本数量越多，该节点的橙色就越深，属于第二类的样本越多，该节点的蓝色就越深。若两类样本的数量相等，则为白色，比如根节点的两类样本数量相同，所以它是白色的</p>
<h3 id="决策树如何应用到数值特征？"><a href="#决策树如何应用到数值特征？" class="headerlink" title="决策树如何应用到数值特征？"></a>决策树如何应用到数值特征？</h3><p>假设有一个数值特征「年龄」，该特征有大量的唯一值。决策树将通过查看「年龄 &lt; 17」、「年龄 &lt; 22.87」这样的二元属性寻找最好的分割，分割的好坏由某种信息增益标准衡量。但在构建树的每一步中，会有过多的二元属性可供选择，比如「薪水」同样能以很多方式进行分割，为了解决这一问题，我们经常使用启发式算法来限制选择的属性数量。</p>
<p>看一个例子，假设有如下数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">data = pd.DataFrame(&#123;<span class="hljs-string">&#x27;Age&#x27;</span>: [<span class="hljs-number">17</span>, <span class="hljs-number">64</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">38</span>, <span class="hljs-number">49</span>, <span class="hljs-number">55</span>, <span class="hljs-number">25</span>, <span class="hljs-number">29</span>, <span class="hljs-number">31</span>, <span class="hljs-number">33</span>],<br>                     <span class="hljs-string">&#x27;Loan Default&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]&#125;)<br>data<br></code></pre></td></tr></table></figure>

<p>使用 <code>sort_values()</code> 方法根据年龄进行升序排列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">data.sort_values(<span class="hljs-string">&#x27;Age&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>训练一个决策树模型，并可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">age_tree = DecisionTreeClassifier(random_state=<span class="hljs-number">17</span>)<br>age_tree.fit(data[<span class="hljs-string">&#x27;Age&#x27;</span>].values.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), data[<span class="hljs-string">&#x27;Loan Default&#x27;</span>].values)<br><br>dot_data = StringIO()<br>export_graphviz(age_tree, feature_names=[<span class="hljs-string">&#x27;Age&#x27;</span>],<br>                out_file=dot_data, filled=<span class="hljs-literal">True</span>)<br>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())<br>Image(value=graph.create_png())<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175325739.png" srcset="/img/loading.gif" lazyload class title="img">

<p>上图可见，该决策树使用以下 5 个值来评估年龄：43.5、19、22.5、30、32。如果你仔细观察，你会发现它们就是目标变量（Loan Default）出现变化（从 1「切换」到 0 或从 0「切换」到 1）时那两个年龄的平均值。比如，一个 38 岁的客户没能偿还贷款（目标变量为 1），而一个 49 岁的客户还贷了（目标变量为 0），那么树使用的评估值就是 38 和 49 的均值，即 43.5。树寻找那些目标变量发生变化的值，以此作为「切割」的阈值。</p>
<p>下面考虑一个更复杂的例子，把「薪水」变量（以千美元每年为单位）加入数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">data2 = pd.DataFrame(&#123;<span class="hljs-string">&#x27;Age&#x27;</span>:  [<span class="hljs-number">17</span>, <span class="hljs-number">64</span>, <span class="hljs-number">18</span>, <span class="hljs-number">20</span>, <span class="hljs-number">38</span>, <span class="hljs-number">49</span>, <span class="hljs-number">55</span>, <span class="hljs-number">25</span>, <span class="hljs-number">29</span>, <span class="hljs-number">31</span>, <span class="hljs-number">33</span>],<br>                      <span class="hljs-string">&#x27;Salary&#x27;</span>: [<span class="hljs-number">25</span>, <span class="hljs-number">80</span>, <span class="hljs-number">22</span>, <span class="hljs-number">36</span>, <span class="hljs-number">37</span>, <span class="hljs-number">59</span>, <span class="hljs-number">74</span>, <span class="hljs-number">70</span>, <span class="hljs-number">33</span>, <span class="hljs-number">102</span>, <span class="hljs-number">88</span>],<br>                      <span class="hljs-string">&#x27;Loan Default&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]&#125;)<br>data2.sort_values(<span class="hljs-string">&#x27;Age&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>上表可见，如果根据年龄排序，目标变量（Loan Default）将切换（从 1 到 0 或从 0 到 1）5 次。</p>
<p>下表可见，如果根据薪水排序，它将切换 7 次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">data2.sort_values(<span class="hljs-string">&#x27;Salary&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>下面看看树将如何选择特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">age_sal_tree = DecisionTreeClassifier(random_state=<span class="hljs-number">17</span>)<br>age_sal_tree.fit(data2[[<span class="hljs-string">&#x27;Age&#x27;</span>, <span class="hljs-string">&#x27;Salary&#x27;</span>]].values, data2[<span class="hljs-string">&#x27;Loan Default&#x27;</span>].values)<br><br>dot_data = StringIO()<br>export_graphviz(age_sal_tree, feature_names=[<span class="hljs-string">&#x27;Age&#x27;</span>, <span class="hljs-string">&#x27;Salary&#x27;</span>],<br>                out_file=dot_data, filled=<span class="hljs-literal">True</span>)<br>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())<br>Image(value=graph.create_png())<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175327325.png" srcset="/img/loading.gif" lazyload class title="img">

<p>上图表明，树同时根据薪水和年龄进行分区，有些节点的分割阈值选择了年龄，有些选择了薪水。树为何选择这些特征？因为根据基尼不确定性质量标准，它们提供了更好的分区。</p>
<p>结论：决策树处理数值特征最简单的启发式算法是升序排列它的值，然后只关注目标变量发生改变的那些值。</p>
<p>此外，当数据集具有大量数值特征，且每个特征具有大量唯一值时，只选择最高的N个阈值，即，仅仅使用提供最高增益的前N个值。这一过程可以看成是构造了一棵深度为 1 的树，计算熵（或基尼不确定性），然后选择最佳阈值用于比较。比方说，如果我们根据「薪水 ≤ 34.5」分割，左子组的熵为 0（所有客户都是「不好的」），而右边的熵为 0.954（3 个「不好的」，5 个「好的」，你可以自行确认这一点，这将作为作业的一部分），信息增益大概是 0.3。如果我们根据「薪水 ≤ 95」分割，左边的子组的熵会是 0.97（6 个「不好的」，4 个「好的」），而右边的熵会是 0（该组只包含 1 个对象），信息增益大约是 0.11。如果以这样的方式计算每种分区的信息增益，那么在使用所有特征构造一棵大决策树之前就可以选出每个数值特征的阈值。</p>
<h3 id="树的关键参数"><a href="#树的关键参数" class="headerlink" title="树的关键参数"></a>树的关键参数</h3><p>理论上讲，我们可以构建一个决策树，直到每个叶节点只有一个实例，但这样做容易过拟合，导致其在新数据上的表现不佳。如果你这么做，在树的最深处，可能会存在由无关紧要的特征组成的分区，例如根据「客户裤子的颜色」这一特征进行分区，这是我们不希望发生。</p>
<p>但在两种情况下，树可以被构建到最大深度（每个叶节点只有一个实例）：</p>
<ul>
<li>随机森林。它将构建为最大深度的单个树的响应进行平均（稍后我们将讨论为什么要这样做）。</li>
<li>决策树修剪。在这种方法中，树首先被构造成最大深度。然后，从底部开始，基于交叉验证来比较有分区&#x2F;无分区情形下树的质量情况，进而移除树的一些节点。</li>
</ul>
<p>下图是过拟合的决<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175328060.png" srcset="/img/loading.gif" lazyload class title="img">21275.png)</p>
<p>常见的解决决策树过拟合的方法为：</p>
<ul>
<li>人工限制深度或叶节点的最少样本数。</li>
<li>对树进行剪枝。</li>
</ul>
<h3 id="scikit-learn-的-DecisionTreeClassifier-类"><a href="#scikit-learn-的-DecisionTreeClassifier-类" class="headerlink" title="scikit-learn 的 DecisionTreeClassifier 类"></a>scikit-learn 的 DecisionTreeClassifier 类</h3><p>sklearn.tree.DecisionTreeClassifier 类的主要参数为：</p>
<ul>
<li>max_depth 树的最大深度；</li>
<li>max_features 搜索最佳分区时的最大特征数（特征很多时，设置这个参数很有必要，因为基于所有特征搜索分区会很「昂贵」）；</li>
<li>min_samples_leaf 叶节点的最少样本数。</li>
</ul>
<p>树的参数需要根据输入数据设定，通常通过交叉验证可以确定参数范围，下文会具体讨论交叉验证。</p>
<h3 id="回归问题中的决策树"><a href="#回归问题中的决策树" class="headerlink" title="回归问题中的决策树"></a>回归问题中的决策树</h3><p>当对数值变量进行预测时，我们构造决策树的思路和分类问题时所用的思路是一样的，但衡量决策树好坏的质量标准改变了，现在它的质量标准如下：</p>
<p>$$D &#x3D; \frac{1}{\ell} \sum\limits_{i &#x3D;1}^{\ell} (y_i - \frac{1}{\ell} \sum\limits_{j&#x3D;1}^{\ell} y_j)^2, $$</p>
<p>其中，ℓℓ 是叶节点中的样本数，𝑦𝑖<em>y**i</em> 是目标变量的值。简单来说，通过最小化方差，使每个叶子中的目标特征的值大致相等，以此来划分训练集的特征。</p>
<h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h4><p>让我们基于以下函数生成一些带噪数据：</p>
<p>$$f(x) &#x3D; e^{-x ^ 2} + 1.5 * e^{-(x - 2) ^ 2}$$</p>
<p>接着在生成的数据上训练一颗决策树，并进行预测，调用 <code>plt</code> 方法画出结果示意图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<br>n_train = <span class="hljs-number">150</span><br>n_test = <span class="hljs-number">1000</span><br>noise = <span class="hljs-number">0.1</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    x = x.ravel()<br>    <span class="hljs-keyword">return</span> np.exp(-x ** <span class="hljs-number">2</span>) + <span class="hljs-number">1.5</span> * np.exp(-(x - <span class="hljs-number">2</span>) ** <span class="hljs-number">2</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">n_samples, noise</span>):<br>    X = np.random.rand(n_samples) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span><br>    X = np.sort(X).ravel()<br>    y = np.exp(-X ** <span class="hljs-number">2</span>) + <span class="hljs-number">1.5</span> * np.exp(-(X - <span class="hljs-number">2</span>) ** <span class="hljs-number">2</span>) + \<br>        np.random.normal(<span class="hljs-number">0.0</span>, noise, n_samples)<br>    X = X.reshape((n_samples, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> X, y<br><br><br>X_train, y_train = generate(n_samples=n_train, noise=noise)<br>X_test, y_test = generate(n_samples=n_test, noise=noise)<br><br><br>reg_tree = DecisionTreeRegressor(max_depth=<span class="hljs-number">5</span>, random_state=<span class="hljs-number">17</span>)<br><br>reg_tree.fit(X_train, y_train)<br>reg_tree_pred = reg_tree.predict(X_test)<br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(X_test, f(X_test), <span class="hljs-string">&quot;b&quot;</span>)<br>plt.scatter(X_train, y_train, c=<span class="hljs-string">&quot;b&quot;</span>, s=<span class="hljs-number">20</span>)<br>plt.plot(X_test, reg_tree_pred, <span class="hljs-string">&quot;g&quot;</span>, lw=<span class="hljs-number">2</span>)<br>plt.xlim([-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>plt.title(<span class="hljs-string">&quot;Decision tree regressor, MSE = %.2f&quot;</span> %<br>          (np.<span class="hljs-built_in">sum</span>((y_test - reg_tree_pred) ** <span class="hljs-number">2</span>) / n_test))<br>plt.show()<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175329058.png" srcset="/img/loading.gif" lazyload class title="image-20240618121026630">

<p>上图表明，决策树使用分段的常数函数逼近数据。</p>
<h3 id="最近邻方法"><a href="#最近邻方法" class="headerlink" title="最近邻方法"></a>最近邻方法</h3><p>最近邻方法（K 近邻或 k-NN）是另一个非常流行的分类方法。当然，也可以用于回归问题。和决策树类似，这是最容易理解的分类方法之一。这一方法遵循紧密性假说：如果样本间的距离能以足够好的方法衡量，那么相似的样本更可能属于同一分类。</p>
<p>比如，根据最近邻方法，下图中的绿球将被分类为「蓝色」而不是「红色」，因为它与蓝球的距离更近。</p>
<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100621566.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>再举一个例子，如果你不知道蓝牙耳机属于什么类别，你可以查找 5 个相似的耳机，如果其中 4 个标记为「配件」类，只有 1 个标记为「科技」类，那么根据最近邻方法，它属于「配件」类。</p>
<p>在最近邻方法中，为了对测试集中的每个样本进行分类，需要依次进行以下操作：</p>
<ul>
<li>计算训练集中每个样本之间的距离。</li>
<li>从训练集中选取 k 个距离最近的样本。</li>
<li>测试样本的类别将是它 k 个最近邻中最常见的分类。</li>
</ul>
<p>在回归问题中应用最近邻方法很简单，仅需将上述步骤做一个小小的改动：第三步不返回分类，而是返回一个数字，即目标变量在邻居中的均值或中位数。</p>
<p>这一方式的显著特点是它具有惰性：当需要对测试样本进行分类时，计算只在预测阶段进行。由于这种特点，最近邻方法事先并不基于训练样本创建模型，这与上文提到的决策树不同。决策树是基于训练集构建的，在预测阶段仅通过遍历决策树就可以实现快速地分类。</p>
<h3 id="最近邻方法的实际应用"><a href="#最近邻方法的实际应用" class="headerlink" title="最近邻方法的实际应用"></a>最近邻方法的实际应用</h3><ul>
<li>在某些案例中，k-NN 可以作为一个模型的基线。</li>
<li>在 Kaggle 竞赛中，k-NN 常常用于构建元特征（即 k-NN 的预测结果作为其他模型的输入），或用于堆叠&#x2F;混合。</li>
<li>最近邻方法还可以扩展到推荐系统等任务中。</li>
<li>在大型数据集上，常常使用逼近方法搜索最近邻。</li>
</ul>
<p>k-NN 分类&#x2F;回归的效果取决于一些参数：</p>
<ul>
<li>邻居数 k。</li>
<li>样本之间的距离度量（常见的包括 Hamming，欧几里得，余弦和 Minkowski 距离）。注意，大部分距离要求数据在同一尺度下，例如「薪水」特征的数值在千级，「年龄」特征的数值却在百级，如果直接将他们丢进最近邻模型中，「年龄」特征就会受到比较大的影响。</li>
<li>邻居的权重（每个邻居可能贡献不同的权重，例如，样本越远，权重越低）。</li>
</ul>
<h3 id="scikit-learn-的-KNeighborsClassifier-类"><a href="#scikit-learn-的-KNeighborsClassifier-类" class="headerlink" title="scikit-learn 的 KNeighborsClassifier 类"></a>scikit-learn 的 KNeighborsClassifier 类</h3><p><code>sklearn.neighbors.KNeighborsClassifier</code> 类的主要参数为：</p>
<ul>
<li>weights：可设为 uniform（所有权重相等），distance（权重和到测试样本的距离成反比），或任何其他用户自定义的函数。</li>
<li>algorithm（可选）：可设为 brute、ball_tree、KD_tree、auto。若设为 brute，通过训练集上的网格搜索来计算每个测试样本的最近邻；若设为 ball_tree 或 KD_tree，样本间的距离储存在树中，以加速寻找最近邻；若设为 auto，将基于训练集自动选择合适的寻找最近邻的方法。</li>
<li>leaf_size（可选）：若寻找最近邻的算法是 BallTree 或 KDTree，则切换为网格搜索所用的阈值。</li>
<li>metric：可设为 minkowski、manhattan、euclidean、chebyshev 或其他。</li>
</ul>
<h3 id="选择模型参数和交叉验证"><a href="#选择模型参数和交叉验证" class="headerlink" title="选择模型参数和交叉验证"></a>选择模型参数和交叉验证</h3><p>机器学习算法的主要任务是可以「泛化」未曾见过的数据。由于我们无法立刻得知模型在新数据上的表现（因为还不知道目标变量的真值），因此有必要牺牲一小部分数据，来验证模型的质量，即将一小部分数据作为留置集。</p>
<p>通常采用下述两种方法之一来验证模型的质量：</p>
<ul>
<li>留置法。保留一小部分数据（一般是 20% 到 40%）作为留置集，在其余数据上训练模型（原数据集的 60%-80%），然后在留置集上验证模型的质量。</li>
<li>交叉验证。最常见的情形是 k 折交叉验证，如下图所示。</li>
</ul>
<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100621716.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>在 k 折交叉验证中，模型在原数据集的 𝐾−1<em>K</em>−1 个子集上进行训练（上图白色部分），然后在剩下的 1 个子集上验证表现，重复训练和验证的过程，每次使用不同的子集（上图橙色部分），总共进行 K 次，由此得到 K 个模型质量评估指数，通常用这些评估指数的求和平均数来衡量分类&#x2F;回归模型的总体质量。</p>
<p>相比留置法，交叉验证能更好地评估模型在新数据上的表现。然而，当你有大量数据时，交叉验证对机器计算能力的要求会变得很高。</p>
<p>交叉验证是机器学习中非常重要的技术，同时也应用于统计学和经济学领域。它有助于我们进行超参数调优、模型比较、特征评估等其他重要操作。你可以点击 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html"> <em>这里</em></a>以了解更多关于交叉验证的信息 。</p>
<h3 id="应用样例"><a href="#应用样例" class="headerlink" title="应用样例"></a>应用样例</h3><h4 id="在客户离网率预测任务中使用决策树和最近邻方法"><a href="#在客户离网率预测任务中使用决策树和最近邻方法" class="headerlink" title="在客户离网率预测任务中使用决策树和最近邻方法"></a>在客户离网率预测任务中使用决策树和最近邻方法</h4><p>首先读取数据至 DataFrame 并进行预处理。将 State 特征从 DateFrame 转移到单独的 Series 对象中。我们训练的第一个模型将不包括 State 特征，之后再考察 State 特征是否有用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.read_csv(<br>    <span class="hljs-string">&#x27;https://labfile.oss.aliyuncs.com/courses/1283/telecom_churn.csv&#x27;</span>)<br><br>df[<span class="hljs-string">&#x27;International plan&#x27;</span>] = pd.factorize(df[<span class="hljs-string">&#x27;International plan&#x27;</span>])[<span class="hljs-number">0</span>]<br>df[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>] = pd.factorize(df[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>])[<span class="hljs-number">0</span>]<br>df[<span class="hljs-string">&#x27;Churn&#x27;</span>] = df[<span class="hljs-string">&#x27;Churn&#x27;</span>].astype(<span class="hljs-string">&#x27;int&#x27;</span>)<br>states = df[<span class="hljs-string">&#x27;State&#x27;</span>]<br>y = df[<span class="hljs-string">&#x27;Churn&#x27;</span>]<br>df.drop([<span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;Churn&#x27;</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<br><br>df.head()<br></code></pre></td></tr></table></figure>

<p>之后将数据集的 70% 划分为训练集（X_train,y_train），30% 划分为留置集（X_holdout,y_holdout）。留置集的数据在调优模型参数时不会被用到，在调优之后，用它评定所得模型的质量。</p>
<p>接下来，训练 2 个模型：决策树和 k-NN。一开始，我们并不知道如何设置模型参数能使模型表现好，所以可以使用随机参数方法，假定树深（max_dept）为 5，近邻数量（n_neighbors）为 10。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split, StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br>X_train, X_holdout, y_train, y_holdout = train_test_split(df.values, y, test_size=<span class="hljs-number">0.3</span>,<br>                                                          random_state=<span class="hljs-number">17</span>)<br><br>tree = DecisionTreeClassifier(max_depth=<span class="hljs-number">5</span>, random_state=<span class="hljs-number">17</span>)<br>knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<br><br>tree.fit(X_train, y_train)<br>knn.fit(X_train, y_train)<br></code></pre></td></tr></table></figure>

<p>使用准确率（Accuracy）在留置集上评价模型预测的质量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br>tree_pred = tree.predict(X_holdout)<br>accuracy_score(y_holdout, tree_pred)<br><br>knn_pred = knn.predict(X_holdout)<br>accuracy_score(y_holdout, knn_pred)<br></code></pre></td></tr></table></figure>

<p>从上可知，决策树的准确率约为 94%，k-NN 的准确率约为 88%，于是仅使用我们假定的随机参数（即没有调参），决策树的表现更好。</p>
<p>现在，使用交叉验证确定树的参数，对每次分割的 max_dept（最大深度 h）和 max_features（最大特征数）进行调优。<code>GridSearchCV()</code> 函数可以非常简单的实现交叉验证，下面程序对每一对 max_depth 和 max_features 的值使用 5 折验证计算模型的表现，接着选择参数的最佳组合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV, cross_val_score<br><br>tree_params = &#123;<span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>, <span class="hljs-number">7</span>),<br>               <span class="hljs-string">&#x27;max_features&#x27;</span>: <span class="hljs-built_in">range</span>(<span class="hljs-number">16</span>, <span class="hljs-number">18</span>)&#125;<br><br>tree_grid = GridSearchCV(tree, tree_params,<br>                         cv=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>, verbose=<span class="hljs-literal">True</span>)<br><br>tree_grid.fit(X_train, y_train)<br></code></pre></td></tr></table></figure>

<p>列出交叉验证得出的最佳参数和相应的训练集准确率均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tree_grid.best_params_<br><br>tree_grid.best_score_<br><br>accuracy_score(y_holdout, tree_grid.predict(X_holdout))<br></code></pre></td></tr></table></figure>

<p>绘制所得的决策树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">dot_data = StringIO()<br>export_graphviz(tree_grid.best_estimator_, feature_names=df.columns,<br>                out_file=dot_data, filled=<span class="hljs-literal">True</span>)<br>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())<br>Image(value=graph.create_png())<br></code></pre></td></tr></table></figure>

<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622196.png" srcset="/img/loading.gif" lazyload></p>
<p>现在，再次使用交叉验证对 k-NN 的 k 值（即邻居数）进行调优。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>knn_pipe = Pipeline([(<span class="hljs-string">&#x27;scaler&#x27;</span>, StandardScaler()),<br>                     (<span class="hljs-string">&#x27;knn&#x27;</span>, KNeighborsClassifier(n_jobs=-<span class="hljs-number">1</span>))])<br><br>knn_params = &#123;<span class="hljs-string">&#x27;knn__n_neighbors&#x27;</span>: <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>, <span class="hljs-number">8</span>)&#125;<br><br>knn_grid = GridSearchCV(knn_pipe, knn_params,<br>                        cv=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>,<br>                        verbose=<span class="hljs-literal">True</span>)<br><br>knn_grid.fit(X_train, y_train)<br><br>knn_grid.best_params_, knn_grid.best_score_<br><br>knn_grid.best_params_<br><br>accuracy_score(y_holdout, knn_grid.predict(X_holdout))<br></code></pre></td></tr></table></figure>

<p>从上可知，在 1-9 范围（range 不包括 10）内最优的 k 值为 7，其交叉验证的准确率约为 88.5%，调优后 k-NN 在留置集上的准确率约为 89%。</p>
<p>综上所述，在这个任务里，决策树有着 94%&#x2F;94.6%（留置法&#x2F;交叉验证调优后）的准确率，k-NN 有着 88%&#x2F;89%（留置法&#x2F;交叉验证调优后）的准确率，显然决策树的表现更好。</p>
<p>使用 <code>RandomForestClassifier()</code> 方法再训练一个随机森林（可以把它想象成一群互相协作的决策树），看看能否在这个任务上有更好的表现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>forest = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, n_jobs=-<span class="hljs-number">1</span>,<br>                                random_state=<span class="hljs-number">17</span>)<br>np.mean(cross_val_score(forest, X_train, y_train, cv=<span class="hljs-number">5</span>))<br><br>forest_params = &#123;<span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-built_in">range</span>(<span class="hljs-number">8</span>, <span class="hljs-number">10</span>),<br>                 <span class="hljs-string">&#x27;max_features&#x27;</span>: <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>, <span class="hljs-number">7</span>)&#125;<br><br>forest_grid = GridSearchCV(forest, forest_params,<br>                           cv=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>, verbose=<span class="hljs-literal">True</span>)<br><br>forest_grid.fit(X_train, y_train)<br>forest_grid.best_params_, forest_grid.best_score_<br><br>accuracy_score(y_holdout, forest_grid.predict(X_holdout))<br></code></pre></td></tr></table></figure>

<p>从上可知，随机森林有着 95.3% 的准确率。不得不说，决策树在这个任务上的表现非常好，即使是训练时间长得多的随机森林也无法取得比它更好的表现。</p>
<h3 id="决策树的复杂情况"><a href="#决策树的复杂情况" class="headerlink" title="决策树的复杂情况"></a>决策树的复杂情况</h3><p>为了继续讨论决策树和 k-NN 的优劣，让我们考虑另外一个简单的分类任务，在这个任务中决策树的表现不错但得到的分类边界过于复杂。</p>
<p>首先，在一个平面上创建一组具有 2 个分类的数据点，每个数据点是两个分类中的一个（红色表示 𝑥1&gt;𝑥2<em>x</em>1&gt;<em>x</em>2，黄色表示 𝑥1&lt;𝑥2<em>x</em>1&lt;<em>x</em>2），其实用一条直线 𝑥1&#x3D;𝑥2<em>x</em>1&#x3D;<em>x</em>2 就可以完成它们的分类，那么决策树会这么做吗？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">form_linearly_separable_data</span>(<span class="hljs-params">n=<span class="hljs-number">500</span>, x1_min=<span class="hljs-number">0</span>, x1_max=<span class="hljs-number">30</span>,</span><br><span class="hljs-params">                                 x2_min=<span class="hljs-number">0</span>, x2_max=<span class="hljs-number">30</span></span>):<br>    data, target = [], []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        x1 = np.random.randint(x1_min, x1_max)<br>        x2 = np.random.randint(x2_min, x2_max)<br>        <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">abs</span>(x1 - x2) &gt; <span class="hljs-number">0.5</span>:<br>            data.append([x1, x2])<br>            target.append(np.sign(x1 - x2))<br>    <span class="hljs-keyword">return</span> np.array(data), np.array(target)<br><br><br>X, y = form_linearly_separable_data()<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=<span class="hljs-string">&#x27;autumn&#x27;</span>, edgecolors=<span class="hljs-string">&#x27;black&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622282.png" srcset="/img/loading.gif" lazyload alt="image-20240618142446682"></p>
<p>训练一个决策树对上面的数据进行分类，并绘制分类边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">tree = DecisionTreeClassifier(random_state=<span class="hljs-number">17</span>).fit(X, y)<br><br>xx, yy = get_grid(X)<br>predicted = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)<br>plt.pcolormesh(xx, yy, predicted, cmap=<span class="hljs-string">&#x27;autumn&#x27;</span>)<br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, s=<span class="hljs-number">100</span>,<br>            cmap=<span class="hljs-string">&#x27;autumn&#x27;</span>, edgecolors=<span class="hljs-string">&#x27;black&#x27;</span>, linewidth=<span class="hljs-number">1.5</span>)<br>plt.title(<span class="hljs-string">&#x27;Easy task. Decision tree compexifies everything&#x27;</span>)<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175335239.png" srcset="/img/loading.gif" lazyload class title="image-20240618142609441">

<p>可视化决策树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">dot_data = StringIO()<br>export_graphviz(tree, feature_names=[<span class="hljs-string">&#x27;x1&#x27;</span>, <span class="hljs-string">&#x27;x2&#x27;</span>],<br>                out_file=dot_data, filled=<span class="hljs-literal">True</span>)<br>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())<br>Image(value=graph.create_png())<br></code></pre></td></tr></table></figure>

<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175335997.png" srcset="/img/loading.gif" lazyload class>

<p>从上可知，决策树构建的边界过于复杂，而且树的深度过深，产生了过拟合现象。</p>
<p>再训练一个 k-NN 模型，看看它在这个任务上的表现情况。该单元格执行时间较长，建议在线下练习：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">knn = KNeighborsClassifier(<span class="hljs-attribute">n_neighbors</span>=1).fit(X, y)<br><br>xx, yy = get_grid(X)<br>predicted = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)<br>plt.pcolormesh(xx, yy, predicted, <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&#x27;autumn&#x27;</span>)<br>plt.scatter(X[:, 0], X[:, 1], <span class="hljs-attribute">c</span>=y, <span class="hljs-attribute">s</span>=100,<br>            <span class="hljs-attribute">cmap</span>=<span class="hljs-string">&#x27;autumn&#x27;</span>, <span class="hljs-attribute">edgecolors</span>=<span class="hljs-string">&#x27;black&#x27;</span>, <span class="hljs-attribute">linewidth</span>=1.5)<br>plt.title(<span class="hljs-string">&#x27;Easy task, kNN. Not bad&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622578.png" srcset="/img/loading.gif" lazyload alt="图片描述"></p>
<p>从上可知，最近邻方法的表现比决策树好一点，但仍然比不上线性分类器 𝑥1&#x3D;𝑥2<em>x</em>1&#x3D;<em>x</em>2（线性分类器将是下一个实验的内容）。</p>
<h3 id="在-MNIST-手写数字识别任务中应用决策树和-k-NN"><a href="#在-MNIST-手写数字识别任务中应用决策树和-k-NN" class="headerlink" title="在 MNIST 手写数字识别任务中应用决策树和 k-NN"></a>在 MNIST 手写数字识别任务中应用决策树和 k-NN</h3><p>现在可以看看这两个算法应用到实际任务上的表现如何，首先载入 sklearn 内置的 MNIST 手写数字数据集，该数据库中手写数字的图片为 8x8 的矩阵，矩阵中的值表示每个像素的白色亮度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits<br><br>data = load_digits()<br>X, y = data.data, data.target<br><br>X[<span class="hljs-number">0</span>, :].reshape([<span class="hljs-number">8</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure>

<p>绘制一些 MNIST 手写数字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">f, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">16</span>, <span class="hljs-number">6</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>    axes[i].imshow(X[i, :].reshape([<span class="hljs-number">8</span>, <span class="hljs-number">8</span>]), cmap=<span class="hljs-string">&#x27;Greys&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>使用 <code>train_test_split()</code> 方法分割数据集，其中的 70% 作为训练集（X_train，y_train），30% 作为留置集（X_holdout，y_holdout）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train, X_holdout, y_train, y_holdout = train_test_split(<br>    X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">17</span>)<br></code></pre></td></tr></table></figure>

<p>使用随机参数训练决策树和 k-NN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tree = DecisionTreeClassifier(max_depth=<span class="hljs-number">5</span>, random_state=<span class="hljs-number">17</span>)<br>knn_pipe = Pipeline([(<span class="hljs-string">&#x27;scaler&#x27;</span>, StandardScaler()),<br>                     (<span class="hljs-string">&#x27;knn&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>))])<br><br>tree.fit(X_train, y_train)<br>knn_pipe.fit(X_train, y_train)<br></code></pre></td></tr></table></figure>

<p>训练好之后，分别在留置集上做出预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tree_pred = tree.predict(X_holdout)<br>knn_pred = knn_pipe.predict(X_holdout)<br>accuracy_score(y_holdout, knn_pred), accuracy_score(<br>    y_holdout, tree_pred)  <span class="hljs-comment"># (0.976, 0.666)</span><br></code></pre></td></tr></table></figure>

<p>从上可知，k-NN 做得更好，不过别忘了我们用的是随机参数。现在，使用交叉验证调优决策树模型，因为这次任务所需考虑的特征比之前任务中的更多，所以可以增加参数的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">tree_params = &#123;<span class="hljs-string">&#x27;max_depth&#x27;</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>],<br>               <span class="hljs-string">&#x27;max_features&#x27;</span>: [<span class="hljs-number">30</span>, <span class="hljs-number">50</span>, <span class="hljs-number">64</span>]&#125;<br><br>tree_grid = GridSearchCV(tree, tree_params,<br>                         cv=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>, verbose=<span class="hljs-literal">True</span>)<br><br>tree_grid.fit(X_train, y_train)<br></code></pre></td></tr></table></figure>

<p>查看交叉验证得到的最佳参数组合和相应的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tree_grid.best_params_, tree_grid.best_score_<br></code></pre></td></tr></table></figure>

<p>调优后决策树模型的准确率达到了 84.4%，但还不到使用随机参数的 k-NN 的准确率（97%）。现在，使用交叉验证调优 k-NN 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean(cross_val_score(KNeighborsClassifier(<br>    n_neighbors=<span class="hljs-number">1</span>), X_train, y_train, cv=<span class="hljs-number">5</span>))<br></code></pre></td></tr></table></figure>

<p>从上可知，调优后的 k-NN 在这一数据集上可以达到 98.7% 的准确率。</p>
<p>下面在这一数据集上训练随机森林模型，在大多数数据集上，它的效果比 k-NN 要好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean(cross_val_score(RandomForestClassifier(<br>    random_state=<span class="hljs-number">17</span>), X_train, y_train, cv=<span class="hljs-number">5</span>))<br></code></pre></td></tr></table></figure>

<p>从上可知，在这个数据集中随机森林的准确率（93.5%）不如 k-NN（98.7%）。当然，我们没有对随机森林的参数进行任何调优，但即使经过调优，训练精确度也无法超过 k-NN。</p>
<p>决策树、k-NN、随机森林在这个数据集上的准确率如下所示：</p>
<table>
<thead>
<tr>
<th>算法\方式</th>
<th>留置法</th>
<th>交叉验证</th>
</tr>
</thead>
<tbody><tr>
<td>决策树</td>
<td>0.667</td>
<td>0.844</td>
</tr>
<tr>
<td>k-NN</td>
<td>0.976</td>
<td>0.987</td>
</tr>
<tr>
<td>随机森林</td>
<td>&#x2F;</td>
<td>0.934</td>
</tr>
</tbody></table>
<p>从这个任务中得到的结论（同时也是一个通用的建议）：首先查看简单模型（决策树、最近邻）在你的数据上的表现，因为可能仅使用简单模型就已经表现得足够好了。</p>
<h3 id="最近邻方法的复杂情形"><a href="#最近邻方法的复杂情形" class="headerlink" title="最近邻方法的复杂情形"></a>最近邻方法的复杂情形</h3><p>下面考虑另一种情况，即在一个分类问题中，某个特征直接和目标变量成比例的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">form_noisy_data</span>(<span class="hljs-params">n_obj=<span class="hljs-number">1000</span>, n_feat=<span class="hljs-number">100</span>, random_seed=<span class="hljs-number">17</span></span>):<br>    np.seed = random_seed<br>    y = np.random.choice([-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], size=n_obj)<br>    <span class="hljs-comment"># 第一个特征与目标成比例</span><br>    x1 = <span class="hljs-number">0.3</span> * y<br>    <span class="hljs-comment"># 其他特征为噪声</span><br>    x_other = np.random.random(size=[n_obj, n_feat - <span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">return</span> np.hstack([x1.reshape([n_obj, <span class="hljs-number">1</span>]), x_other]), y<br><br><br>X, y = form_noisy_data()<br></code></pre></td></tr></table></figure>

<p>使用最近邻方法训练模型后，查看交叉验证和留置集的准确率，并绘制这两个准确率随 n_neighbors 最近邻数目 参数变化的曲线，这样的曲线被称为验证曲线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br>X_train, X_holdout, y_train, y_holdout = train_test_split(<br>    X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">17</span>)<br><br><br>cv_scores, holdout_scores = [], []<br>n_neighb = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>] + <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>, <span class="hljs-number">550</span>, <span class="hljs-number">50</span>))<br><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> n_neighb:<br><br>    knn_pipe = Pipeline([(<span class="hljs-string">&#x27;scaler&#x27;</span>, StandardScaler()),<br>                         (<span class="hljs-string">&#x27;knn&#x27;</span>, KNeighborsClassifier(n_neighbors=k))])<br>    cv_scores.append(np.mean(cross_val_score(<br>        knn_pipe, X_train, y_train, cv=<span class="hljs-number">5</span>)))<br>    knn_pipe.fit(X_train, y_train)<br>    holdout_scores.append(accuracy_score(<br>        y_holdout, knn_pipe.predict(X_holdout)))<br><br>plt.plot(n_neighb, cv_scores, label=<span class="hljs-string">&#x27;CV&#x27;</span>)<br>plt.plot(n_neighb, holdout_scores, label=<span class="hljs-string">&#x27;holdout&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Easy task. kNN fails&#x27;</span>)<br>plt.legend()<br></code></pre></td></tr></table></figure>

<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622695.png" srcset="/img/loading.gif" lazyload alt="image-20240618143637202"></p>
<p>上图表明，即使我们尝试在较广范围内改变 n_neighbors 参数，基于欧几里得距离的 k-NN 在这个问题上依旧表现不佳。</p>
<p>下面用决策树训练一个模型，看看它在这个任务上的表现如何。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tree = DecisionTreeClassifier(random_state=<span class="hljs-number">17</span>, max_depth=<span class="hljs-number">1</span>)<br>tree_cv_score = np.mean(cross_val_score(tree, X_train, y_train, cv=<span class="hljs-number">5</span>))<br>tree.fit(X_train, y_train)<br>tree_holdout_score = accuracy_score(y_holdout, tree.predict(X_holdout))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Decision tree. CV: &#123;&#125;, holdout: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>    tree_cv_score, tree_holdout_score))<br></code></pre></td></tr></table></figure>

<p>在这一任务中，决策树完美地解决了问题，在交叉验证和留置集上都得到了 100% 的准确率。其实，k-NN 之所以在这个任务上表现不佳并非该方法本身的问题，而是因为使用了欧几里得距离，因为欧几里得距离没能察觉出有一个特征（成比例）比其他所有特征（噪声）更重要。</p>
<h3 id="决策树和最近邻方法的优势和劣势"><a href="#决策树和最近邻方法的优势和劣势" class="headerlink" title="决策树和最近邻方法的优势和劣势"></a>决策树和最近邻方法的优势和劣势</h3><h4 id="决策树-1"><a href="#决策树-1" class="headerlink" title="决策树"></a>决策树</h4><p>优势：</p>
<ul>
<li>生成容易理解的分类规则，这一属性称为模型的可解释性。例如它生成的规则可能是「如果年龄不满 25 岁，并对摩托车感兴趣，那么就拒绝发放贷款」。</li>
<li>很容易可视化，即模型本身（树）和特定测试对象的预测（穿过树的路径）可以「被解释」。</li>
<li>训练和预测的速度快。</li>
<li>较少的参数数目。</li>
<li>支持数值和类别特征。</li>
</ul>
<p>劣势：</p>
<ul>
<li>决策树对输入数据中的噪声非常敏感，这削弱了模型的可解释性。</li>
<li>决策树构建的边界有其局限性：它由垂直于其中一个坐标轴的超平面组成，在实践中比其他方法的效果要差。</li>
<li>我们需要通过剪枝、设定叶节点的最小样本数、设定树的最大深度等方法避免过拟合。</li>
<li>不稳定性，数据的细微变动都会显著改变决策树。这一问题可通过决策树集成方法来处理（以后的实验会介绍）。</li>
<li>搜索最佳决策树是一个「NP 完全」（NP-Complete）问题。了解什么是 NP-Complete 请点击 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://baike.baidu.com/item/NP-Complete/15961931?fr=aladdin"> <em>这里</em></a>。实践中使用的一些推断方法，比如基于最大信息增益进行贪婪搜索，并不能保证找到全局最优决策树。</li>
<li>倘若数据中出现缺失值，将难以创建决策树模型。Friedman 的 CART 算法中大约 50% 的代码是为了处理数据中的缺失值（现在 sklearn 实现了这一算法的改进版本）。</li>
<li>这一模型只能内插，不能外推（随机森林和树提升方法也是如此）。也就是说，倘若你预测的对象在训练集所设置的特征空间之外，那么决策树就只能做出常数预测。比如，在我们的黄球和蓝球的例子中，这意味着模型将对所有位于 &gt;19 或 &lt;0 的球做出同样的预测。</li>
</ul>
<h4 id="最近邻方法-1"><a href="#最近邻方法-1" class="headerlink" title="最近邻方法"></a>最近邻方法</h4><p>优势：</p>
<ul>
<li>实现简单。</li>
<li>研究很充分。</li>
<li>通常而言，在分类、回归、推荐问题中第一个值得尝试的方法就是最近邻方法。</li>
<li>通过选择恰当的衡量标准或核，它可以适应某一特定问题。</li>
</ul>
<p>劣势：</p>
<ul>
<li>和其他复合算法相比，这一方法速度较快。但是，现实生活中，用于分类的邻居数目通常较大（100-150），在这一情形下，k-NN 不如决策树快。</li>
<li>如果数据集有很多变量，很难找到合适的权重，也很难判定哪些特征对分类&#x2F;回归不重要。</li>
<li>依赖于对象之间的距离度量，默认选项欧几里得距离常常是不合理的。你可以通过网格搜索参数得到良好的解，但在大型数据集上的耗时很长。</li>
<li>没有理论来指导我们如何选择邻居数，故而只能进行网格搜索（尽管基本上所有的模型，在对其超参数进行调整时都使用网格搜索的方法）。在邻居数较小的情形下，该方法对离散值很敏感，也就是说，有过拟合的倾向。</li>
<li>由于「维度的诅咒」，当数据集存在很多特征时它的表现不佳。</li>
</ul>
<h3 id="实验总结-2"><a href="#实验总结-2" class="headerlink" title="实验总结"></a>实验总结</h3><p>本次实验我们通过决策树和最近邻方法在几个简单示例上构建了分类模型，并基于交叉验证的方法对模型进行调优，之后对比了决策树和最近邻方法的优劣情况。这两种方法常常作为机器学习模型的基线，熟悉他们的用法是十分必要的。</p>
<p> <em>相关链接</em></p>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://scikit-learn.org/stable/documentation.html"> <em>scikit-learn 文档</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/amueller/scipy-2017-sklearn"> <em>scikit-learn 指南</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/diefimov/MTH594_MachineLearning"> <em>机器学习优秀课程</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/rushter/MLAlgorithms"> <em>适合决策树和 k-NN 的机器学习算法实现</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/louplus/"> <em>了解蓝桥云课《楼+ 机器学习和数据挖掘课程》</em></a></li>
</ul>
<h2 id="线性回归和线性分类器"><a href="#线性回归和线性分类器" class="headerlink" title="线性回归和线性分类器"></a>线性回归和线性分类器</h2><hr>
<h4 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h4><p>本次实验简述了最小二乘法、最大似然估计、逻辑回归、正则化、验证和学习曲线的基本概念，搭建了基于逻辑回归的线性模型并进行正则化，通过分析 IBMD 数据集的二元分类问题和一个 XOR 问题阐述逻辑回归的优缺点。</p>
<h4 id="知识点-3"><a href="#知识点-3" class="headerlink" title="知识点"></a>知识点</h4><ul>
<li>回归</li>
<li>线性分类</li>
<li>逻辑回归的正则化</li>
<li>逻辑回归的优缺点</li>
<li>验证和学习曲线</li>
</ul>
<hr>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>在开始学习线性模型之前，简要介绍一下线性回归，首先指定一个模型将因变量 𝑦<em>y</em> 和特征联系起来，对线性模型而言，依赖函数的形式如下：</p>
<p>𝑦&#x3D;𝑤0+∑𝑖&#x3D;1𝑚𝑤𝑖𝑥𝑖<em>y</em>&#x3D;<em>w</em>0+<em>i</em>&#x3D;1∑<em>m<strong>w</strong>i<strong>x</strong>i</em></p>
<p>如果为每项观测加上一个虚维度 𝑥0&#x3D;1<em>x</em>0&#x3D;1（比如偏置），那么就可以把 𝑤0<em>w</em>0 整合进求和项中，改写为一个略微紧凑的形式：</p>
<p>𝑦&#x3D;∑𝑖&#x3D;0𝑚𝑤𝑖𝑥𝑖&#x3D;wTx<em>y</em>&#x3D;<em>i</em>&#x3D;0∑<em>m<strong>w</strong>i<strong>x</strong>i</em>&#x3D;<strong>w</strong>T<strong>x</strong></p>
<p>如果有一个特征观测矩阵，其中矩阵的行是数据集中的观测，那么需要在左边加上一列。由此，线性模型可以定义为：</p>
<p>$$ \textbf y &#x3D; \textbf X \textbf w + \epsilon$$</p>
<p>其中：</p>
<ul>
<li>$\textbf y \in \mathbb{R}^n$：因变量（目标变量）。</li>
<li>$w$：模型的参数向量（在机器学习中，这些参数经常被称为权重）。</li>
<li>$\textbf X$：观测及其特征矩阵，大小为 n 行、m+1 列（包括左侧的虚列），其秩的大小为 $\text{rank}\left(\textbf X\right) &#x3D; m + 1 $。</li>
<li>$\epsilon $：一个变量，用来表示随机、不可预测模型的错误。</li>
</ul>
<p>上述表达式亦可这样写：</p>
<p>$$ y_i &#x3D; \sum_{j&#x3D;0}^m w_j X_{ij} + \epsilon_i$$</p>
<p>模型具有如下限制（否则它就不是线性回归了）：</p>
<ul>
<li>随机误差的期望为零：$\forall i: \mathbb{E}\left[\epsilon_i\right] &#x3D; 0 $;</li>
<li>随机误差具有相同的有限方差，这一性质称为等分散性：$\forall i: \text{Var}\left(\epsilon_i\right) &#x3D; \sigma^2 &lt; \infty $;</li>
<li>随机误差不相关：$\forall i \neq j: \text{Cov}\left(\epsilon_i, \epsilon_j\right) &#x3D; 0 $.</li>
</ul>
<p>权重 $w_i$ 的估计 $\widehat{w}_i$  满足如下条件时，称其为线性：</p>
<p>$$ \widehat{w}<em>i &#x3D; \omega</em>{1i}y_1 + \omega_{2i}y_2 + \cdots + \omega_{ni}y_n$$</p>
<p>其中对于 $\forall\ k\ $，$\omega_{ki}$ 仅依赖于 $X$ 中的样本。由于寻求最佳权重的解是一个线性估计，这一模型被称为线性回归。</p>
<p>再引入一项定义：当期望值等于估计参数的真实值时，权重估计被称为无偏（unbiased）：</p>
<p>$$ \mathbb{E}\left[\widehat{w}_i\right] &#x3D; w_i$$</p>
<p>计算这些权重的方法之一是普通最小二乘法（OLS）。OLS 可以最小化因变量实际值和模型给出的预测值之间的均方误差：</p>
<p>$$ \begin{array}{rcl}\mathcal{L}\left(\textbf X, \textbf{y}, \textbf{w} \right) &amp;&#x3D;&amp; \frac{1}{2n} \sum_{i&#x3D;1}^n \left(y_i - \textbf{w}^\text{T} \textbf{x}_i\right)^2 \ &amp;&#x3D;&amp; \frac{1}{2n} \left| \textbf{y} - \textbf X \textbf{w} \right|_2^2 \ &amp;&#x3D;&amp; \frac{1}{2n} \left(\textbf{y} - \textbf X \textbf{w}\right)^\text{T} \left(\textbf{y} - \textbf X \textbf{w}\right) \end{array}$$</p>
<p>为了解决这一优化问题，需要计算模型参数的导数。将导数设为零，然后求解关于 w<strong>w</strong> 的等式，倘若不熟悉矩阵求导，可以参考下面的 4 个式子：</p>
<p>$$\begin{array}{rcl}<br>\frac{\partial}{\partial \textbf{X}} \textbf{X}^{\text{T}} \textbf{A} &amp;&#x3D;&amp; \textbf{A} \end{array}$$</p>
<p>$$\begin{array}{rcl} \frac{\partial}{\partial \textbf{X}} \textbf{X}^{\text{T}} \textbf{A} \textbf{X} &amp;&#x3D;&amp; \left(\textbf{A} + \textbf{A}^{\text{T}}\right)\textbf{X} \end{array}$$</p>
<p>$$\begin{array}{rcl}\frac{\partial}{\partial \textbf{A}} \textbf{X}^{\text{T}} \textbf{A} \textbf{y} &amp;&#x3D;&amp;  \textbf{X}^{\text{T}} \textbf{y} \end{array}$$</p>
<p>$$\begin{array}{rcl} \frac{\partial}{\partial \textbf{X}} \textbf{A}^{-1} &amp;&#x3D;&amp; -\textbf{A}^{-1} \frac{\partial \textbf{A}}{\partial \textbf{X}} \textbf{A}^{-1}<br>\end{array}$$</p>
<p>现在开始计算模型参数的导数：</p>
<p>$$ \begin{array}{rcl} \frac{\partial \mathcal{L}}{\partial \textbf{w}} &amp;&#x3D;&amp; \frac{\partial}{\partial \textbf{w}} \frac{1}{2n} \left( \textbf{y}^{\text{T}} \textbf{y} -2\textbf{y}^{\text{T}} \textbf{X} \textbf{w} + \textbf{w}^{\text{T}} \textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) \ &amp;&#x3D;&amp; \frac{1}{2n} \left(-2 \textbf{X}^{\text{T}} \textbf{y} + 2\textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) \end{array}$$</p>
<p>$$ \begin{array}{rcl} \frac{\partial \mathcal{L}}{\partial \textbf{w}} &#x3D; 0 &amp;\Leftrightarrow&amp; \frac{1}{2n} \left(-2 \textbf{X}^{\text{T}} \textbf{y} + 2\textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) &#x3D; 0 \ &amp;\Leftrightarrow&amp; -\textbf{X}^{\text{T}} \textbf{y} + \textbf{X}^{\text{T}} \textbf{X} \textbf{w} &#x3D; 0 \ &amp;\Leftrightarrow&amp; \textbf{X}^{\text{T}} \textbf{X} \textbf{w} &#x3D; \textbf{X}^{\text{T}} \textbf{y} \ &amp;\Leftrightarrow&amp; \textbf{w} &#x3D; \left(\textbf{X}^{\text{T}} \textbf{X}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y} \end{array}$$</p>
<p>基于上述的定义和条件，可以说，根据高斯-马尔可夫定理，模型参数的 OLS 估计是所有线性无偏估计中最优的，即通过 OLS 估计可以获得最低的方差。</p>
<p>有人可能会问，为何选择最小化均方误差而不是其他指标？因为若不选择最小化均方误差，那么就不满足高斯-马尔可夫定理的条件，得到的估计将不再是最佳的线性无偏估计。</p>
<p>最大似然估计是解决线性回归问题一种常用方法，下面介绍它的概念。</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>首先举一个简单的例子，我们想做一个试验判定人们是否记得简单的甲醇化学式 𝐶𝐻3𝑂𝐻<em>C**H</em>3<em>O**H</em>。首先调查了 400 人，发现只有 117 个人记得甲醇的化学式。那么，直接将 117400≈29400117≈29 作为估计下一个受访者知道甲醇化学式的概率是较为合理的。这个直观的估计就是一个最大似然估计。为什么会这么估计呢？回忆下伯努利分布的定义：如果一个随机变量只有两个值（1 和 0，相应的概率为 𝜃<em>θ</em> 和 1−𝜃1−<em>θ</em>），那么该随机变量满足伯努利分布，遵循以下概率分布函数：</p>
<p>$$ p\left(\theta, x\right) &#x3D; \theta^x \left(1 - \theta\right)^\left(1 - x\right), x \in \left{0, 1\right}$$</p>
<p>这一分布正是我们所需要的，分布参数 𝜃<em>θ</em> 就是「某个人知道甲醇化学式」的概率估计。在 400 个独立试验中，试验的结果记为 $\textbf{x} &#x3D; \left(x_1, x_2, \ldots, x_{400}\right)$。写下数据的似然，即观测的可能性，比如正好观测到 117 个随机变量 𝑥&#x3D;1<em>x</em>&#x3D;1 和 283 个随机变量 𝑥&#x3D;0<em>x</em>&#x3D;0 的可能性：</p>
<p> $$ p(\textbf{x}; \theta) &#x3D; \prod_{i&#x3D;1}^{400} \theta^{x_i} \left(1 - \theta\right)^{\left(1 - x_i\right)} &#x3D; \theta^{117} \left(1 - \theta\right)^{283}$$</p>
<p>接着，将最大化这一 $\theta$ 的表达式。一般而言，为了简化计算，并不最大化似然 $p(\textbf{x}; \theta)$，转而最大化其对数（这种变换不影响最终答案）：</p>
<p>$$ \log p(\textbf{x}; \theta) &#x3D; \log \prod_{i&#x3D;1}^{400} \theta^{x_i} \left(1 - \theta\right)^{\left(1 - x_i\right)} &#x3D; $$</p>
<p>$$  &#x3D; \log \theta^{117} \left(1 - \theta\right)^{283} &#x3D;  117 \log \theta + 283 \log \left(1 - \theta\right)$$</p>
<p>为了找到最大化上式的 𝜃<em>θ</em> 值，将上式对 𝜃<em>θ</em> 求导，并令其为零，求解所得等式：</p>
<p>$$  \frac{\partial \log p(\textbf{x}; \theta)}{\partial \theta} &#x3D; \frac{\partial}{\partial \theta} \left(117 \log \theta + 283 \log \left(1 - \theta\right)\right) &#x3D; \frac{117}{\theta} - \frac{283}{1 - \theta};$$</p>
<p>由上可知，我们的直观估计正好是最大似然估计。现在将这一推理过程应用到线性回归问题上，尝试找出均方误差背后的道理。为此，需要从概率论的角度来看线性回归。我们的模型和之前是一样的：</p>
<p>$$ \textbf y &#x3D; \textbf X \textbf w + \epsilon$$</p>
<p>不过，现在假定随机误差符合均值为零的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://baike.baidu.com/item/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/829892?fr=aladdin"> <em>正态分布</em></a>：</p>
<p>$$ \epsilon_i \sim \mathcal{N}\left(0, \sigma^2\right)$$</p>
<p>据此改写模型：</p>
<p>$$ \begin{array}{rcl} y_i &amp;&#x3D;&amp; \sum_{j&#x3D;1}^m w_j X_{ij} + \epsilon_i \ &amp;\sim&amp; \sum_{j&#x3D;1}^m w_j X_{ij} + \mathcal{N}\left(0, \sigma^2\right) \ p\left(y_i \mid \textbf X; \textbf{w}\right) &amp;&#x3D;&amp; \mathcal{N}\left(\sum_{j&#x3D;1}^m w_j X_{ij}, \sigma^2\right) \end{array}$$</p>
<p>由于样本是独立抽取的（误差不相关是高斯-马尔可夫定理的条件之一），数据的似然看起来会是密度函数 𝑝(𝑦𝑖)<em>p</em>(<em>y**i</em>) 的积。转化为对数形式：</p>
<p>$$ \begin{array}{rcl} \log p\left(\textbf{y}\mid \textbf X; \textbf{w}\right) &amp;&#x3D;&amp; \log \prod_{i&#x3D;1}^n \mathcal{N}\left(\sum_{j&#x3D;1}^m w_j X_{ij}, \sigma^2\right) \ &amp;&#x3D;&amp; \sum_{i&#x3D;1}^n \log \mathcal{N}\left(\sum_{j&#x3D;1}^m w_j X_{ij}, \sigma^2\right) \ &amp;&#x3D;&amp; -\frac{n}{2}\log 2\pi\sigma^2 -\frac{1}{2\sigma^2} \sum_{i&#x3D;1}^n \left(y_i - \textbf{w}^\text{T} \textbf{x}_i\right)^2 \end{array}$$</p>
<p>想要找到最大似然假设，即需要最大化表达式 𝑝(y∣X;w)<em>p</em>(<strong>y</strong>∣<strong>X</strong>;<strong>w</strong>) 以得到 wML<strong>w</strong>ML，这和最大化其对数是一回事。注意，当针对某个参数最大化函数时，可以丢弃所有不依赖这一参数的变量：</p>
<p>$$ \begin{array}{rcl}<br>\textbf{w}<em>{\text{ML}} &amp;&#x3D;&amp; \arg \max</em>{\textbf w} p\left(\textbf{y}\mid \textbf X; \textbf{w}\right) &#x3D; \arg \max_{\textbf w} \log p\left(\textbf{y}\mid \textbf X; \textbf{w}\right)\<br>&amp;&#x3D;&amp; \arg \max_{\textbf w} -\frac{n}{2}\log 2\pi\sigma^2 -\frac{1}{2\sigma^2} \sum_{i&#x3D;1}^n \left(y_i - \textbf{w}^{\text{T}} \textbf{x}<em>i\right)^2 \<br>&amp;&#x3D;&amp; \arg \max</em>{\textbf w} -\frac{1}{2\sigma^2} \sum_{i&#x3D;1}^n \left(y_i - \textbf{w}^{\text{T}} \textbf{x}<em>i\right)^2 \<br>&amp;&#x3D;&amp;  \arg \min</em>{\textbf w} \mathcal{L}\left(\textbf X, \textbf{y}, \textbf{w} \right)<br>\end{array}$$</p>
<p>所以，当测量误差服从正态（高斯）分布的情况下， 最小二乘法等价于极大似然估计。</p>
<h3 id="偏置-方差分解"><a href="#偏置-方差分解" class="headerlink" title="偏置-方差分解"></a>偏置-方差分解</h3><p>下面讨论线性回归预测的误差性质（可以推广到机器学习算法上），上文提到：</p>
<ul>
<li>目标变量的真值 $y$ 是确定性函数 $f\left(\textbf{x}\right)$ 和随机误差 $\epsilon$ 之和：$y &#x3D; f\left(\textbf{x}\right) + \epsilon$。</li>
<li>误差符合均值为零、方差一致的正态分布：$\epsilon \sim \mathcal{N}\left(0, \sigma^2\right)$。</li>
<li>目标变量的真值亦为正态分布：$y \sim \mathcal{N}\left(f\left(\textbf{x}\right), \sigma^2\right)$。</li>
<li>试图使用一个协变量线性函数逼近一个未知的确定性函数 $f\left(\textbf{x}\right)$，这一协变量线性函数是函数空间中估计函数 $f$ 的一点，即均值和方差的随机变量。</li>
</ul>
<p>因此，点 x<strong>x</strong> 的误差可分解为：</p>
<p>$$ \begin{array}{rcl} \text{Err}\left(\textbf{x}\right) &amp;&#x3D;&amp; \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \ &amp;&#x3D;&amp; \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\left(\widehat{f}\left(\textbf{x}\right)\right)^2\right] - 2\mathbb{E}\left[y\widehat{f}\left(\textbf{x}\right)\right] \ &amp;&#x3D;&amp; \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\widehat{f}^2\right] - 2\mathbb{E}\left[y\widehat{f}\right] \ \end{array}$$</p>
<p>为了简洁，省略函数的参数，分别考虑每个变量。根据公式 Var(𝑧)&#x3D;𝐸[𝑧2]−𝐸[𝑧]2Var(<em>z</em>)&#x3D;E[<em>z</em>2]−E[<em>z</em>]2 可以分解前两项为：</p>
<p>$$ \begin{array}{rcl} \mathbb{E}\left[y^2\right] &amp;&#x3D;&amp; \text{Var}\left(y\right) + \mathbb{E}\left[y\right]^2 &#x3D; \sigma^2 + f^2\ \mathbb{E}\left[\widehat{f}^2\right] &amp;&#x3D;&amp; \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 \ \end{array}$$</p>
<p>注意：</p>
<p>$$ \begin{array}{rcl} \text{Var}\left(y\right) &amp;&#x3D;&amp; \mathbb{E}\left[\left(y - \mathbb{E}\left[y\right]\right)^2\right] \ &amp;&#x3D;&amp; \mathbb{E}\left[\left(y - f\right)^2\right] \ &amp;&#x3D;&amp; \mathbb{E}\left[\left(f + \epsilon - f\right)^2\right] \ &amp;&#x3D;&amp; \mathbb{E}\left[\epsilon^2\right] &#x3D; \sigma^2 \end{array}$$</p>
<p>𝐸[𝑦]&#x3D;𝐸[𝑓+𝜖]&#x3D;𝐸[𝑓]+𝐸[𝜖]&#x3D;𝑓E[<em>y</em>]&#x3D;E[<em>f</em>+<em>ϵ</em>]&#x3D;E[<em>f</em>]+E[<em>ϵ</em>]&#x3D;<em>f</em></p>
<p>接着处理和的最后一项。由于误差和目标变量相互独立，所以可以将它们分离，写为：</p>
<p>$$ \begin{array}{rcl} \mathbb{E}\left[y\widehat{f}\right] &amp;&#x3D;&amp; \mathbb{E}\left[\left(f + \epsilon\right)\widehat{f}\right] \ &amp;&#x3D;&amp; \mathbb{E}\left[f\widehat{f}\right] + \mathbb{E}\left[\epsilon\widehat{f}\right] \ &amp;&#x3D;&amp; f\mathbb{E}\left[\widehat{f}\right] + \mathbb{E}\left[\epsilon\right] \mathbb{E}\left[\widehat{f}\right] &#x3D; f\mathbb{E}\left[\widehat{f}\right] \end{array}$$</p>
<p>最后，将上述公式合并为：</p>
<p>$$ \begin{array}{rcl} \text{Err}\left(\textbf{x}\right) &amp;&#x3D;&amp; \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \ &amp;&#x3D;&amp; \sigma^2 + f^2 + \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 - 2f\mathbb{E}\left[\widehat{f}\right] \ &amp;&#x3D;&amp; \left(f - \mathbb{E}\left[\widehat{f}\right]\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2 \ &amp;&#x3D;&amp; \text{Bias}\left(\widehat{f}\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2 \end{array}$$</p>
<p>由此，从上等式可知，任何线性模型的预测误差由三部分组成：</p>
<ul>
<li>偏差（bias）: $\text{Bias}\left(\widehat{f}\right)$ 度量了学习算法的期望输出与真实结果的偏离程度, 刻画了算法的拟合能力，偏差偏高表示预测函数与真实结果差异很大。</li>
<li>方差（variance）: $\text{Var}\left(\widehat{f}\right)$ 代表「同样大小的不同的训练数据集训练出的模型」与「这些模型的期望输出值」之间的差异。训练集变化导致性能变化，方差偏高表示模型很不稳定。</li>
<li>不可消除的误差（irremovable error）: $\sigma^2$ 刻画了当前任务任何算法所能达到的期望泛化误差的下界，即刻画了问题本身的难度。</li>
</ul>
<p>尽管无法消除 $\sigma^2$，但我们可以影响前两项。理想情况下，希望同时消除偏差和方差（见下图中左上），但是在实践中，常常需要在偏置和不稳定（高方差）间寻找平衡。</p>
<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622792.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>一般而言，当模型的计算量增加时（例如，自由参数的数量增加了），估计的方差（分散程度）也会增加，但偏置会下降，这可能会导致过拟合现象。另一方面，如果模型的计算量太少（例如，自由参数过低)，这可能会导致欠拟合现象。</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175339427.png" srcset="/img/loading.gif" lazyload class title="img">

<p>高斯-马尔可夫定理表明：在线性模型参数估计问题中，OLS 估计是最佳的线性无偏估计。这意味着，如果存在任何无偏线性模型 g，可以确信 $Var\left(\widehat{f}\right) \leq Var\left(g\right)$。</p>
<h3 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h3><p>低偏置和低方差往往是不可兼得的，所以在一些情形下，会为了稳定性（降低模型的方差）而导致模型的偏置 $\text{Var}\left(\widehat{f}\right)$ 提高。高斯-马尔可夫定理成立的条件之一就是矩阵 $\textbf{X}$ 是满秩的，否则 OLS 的解 $\textbf{w} &#x3D; \left(\textbf{X}^\text{T} \textbf{X}\right)^{-1} \textbf{X}^\text{T} \textbf{y}$ 就不存在，因为逆矩阵 $\left(\textbf{X}^\text{T} \textbf{X}\right)^{-1}$ 不存在，此时矩阵 $\textbf{X}^\text{T} \textbf{X}$ 被称为奇异矩阵或退化矩阵。这类问题被称为病态问题，必须加以矫正，也就是说，矩阵 $\textbf{X}^\text{T} \textbf{X}$ 需要变成非奇异矩阵（这正是这一过程叫做正则化的原因）。</p>
<p>我们常常能在这类数据中观察到所谓的多重共线性：两个或更多特征高度相关，也就是矩阵 X<strong>X</strong> 的列之间存在类似线性依赖的关系（又不完全是线性依赖）。例如，在「基于特征预测房价」这一问题中，属性「含阳台的面积」和「不含阳台的面积」会有一个接近线性依赖的关系。数学上，包含这类数据的矩阵 XTX<strong>X</strong>T<strong>X</strong> 被称为可逆矩阵，但由于多重共线性，一些本征值（特征值）会接近零。在 XTX<strong>X</strong>T<strong>X</strong> 的逆矩阵中，因为其本征值为 1𝜆𝑖<em>λ**i</em>1，所以有些本征值会变得特别大。本征值这种巨大的数值波动会导致模型参数估计的不稳定，即在训练数据中加入一组新的观测会导致完全不同的解。为了解决上述问题，有一种正则化的方法称为吉洪诺夫（Tikhonov）正则化，大致上是在均方误差中加上一个新变量：</p>
<p>$$ \begin{array}{rcl} \mathcal{L}\left(\textbf{X}, \textbf{y}, \textbf{w} \right) &amp;&#x3D;&amp; \frac{1}{2n} \left| \textbf{y} - \textbf{X} \textbf{w} \right|_2^2 + \left| \Gamma \textbf{w}\right|^2\end{array}$$</p>
<p>吉洪诺夫矩阵常常表达为单位矩阵乘上一个系数：$\Gamma &#x3D; \frac{\lambda}{2} E$。在这一情形下，最小化均方误差问题变为一个 L2 正则化问题。若对新的损失函数求导，设所得函数为零，据  $\textbf{w}$ 重整等式，便得到了这一问题的解：</p>
<p>$$ \begin{array}{rcl}<br>\textbf{w} &amp;&#x3D;&amp; \left(\textbf{X}^{\text{T}} \textbf{X} + \lambda \textbf{E}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}<br>\end{array}$$</p>
<p>这类回归被称为岭回归（ridge regression）。岭为对角矩阵，在 $\textbf{X}^\text{T} \textbf{X}$ 矩阵上加上这一对角矩阵，以确保能得到一个正则矩阵。</p>
<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100622978.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>这样的解降低了方差，但增加了偏置，因为参数的正则向量也被最小化了，这导致解朝零移动。在下图中，OLS 解为白色虚线的交点，蓝点表示岭回归的不同解。可以看到，通过增加正则化参数 $\lambda$，使解朝零移动。</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175342146.png" srcset="/img/loading.gif" lazyload class title="img">

<h3 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h3><p>线性分类器背后的基本思路是，目标分类的值可以被特征空间中的一个超平面分开。如果这可以无误差地达成，那么训练集被称为线性可分。</p>
<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100623178.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>上面已经介绍了线性回归和普通最小二乘法（OLS）。现在考虑一个二元分类问题，将目标分类记为「+1」（正面样本）和「-1」（负面样本）。最简单的线性分类器可以通过回归定义：</p>
<p>$$ a(\textbf{x}) &#x3D; \text{sign}(\textbf{w}^\text{T}\textbf x)$$</p>
<p>其中：</p>
<ul>
<li>$\textbf{x}$ 是特征向量（包括标识）。</li>
<li>$\textbf{w}$ 是线性模型中的权重向量（偏置为 $w_0$）。</li>
<li>$\text{sign}(\bullet)$ 是符号函数，返回参数的符号。</li>
<li>$a(\textbf{x})$ 是分类 $\textbf{x}$ 的分类器。</li>
</ul>
<h3 id="基于逻辑回归的线性分类器"><a href="#基于逻辑回归的线性分类器" class="headerlink" title="基于逻辑回归的线性分类器"></a>基于逻辑回归的线性分类器</h3><p>逻辑回归是线性分类器的一个特殊情形，但逻辑回归有一个额外的优点：它可以预测样本 $\textbf{x}<em>\text{i}$ 为分类「+」的概率 $p</em>+$：</p>
<p>$$ p_+ &#x3D; P\left(y_i &#x3D; 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) $$</p>
<p>逻辑回归不仅能够预测样本是「+1」还是「-1」，还能预测其分别是「+1」和「-1」的概率是多少。对于很多业务问题（比如，信用评分问题）而言，这是一个非常重要的优点。下面是一个预测贷款违约概率的例子。</p>
<p><img src="/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91index/IMG-20240814100623265.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>银行选择一个阈值 $p_*$ 以预测贷款违约的概率（上图中阈值为0.15），超过阈值就不批准贷款。</p>
<p>为了预测概率 $p_+ \in [0,1]$，使用 OLS 构造线性预测：</p>
<p>$$b(\textbf{x}) &#x3D; \textbf{w}^\text{T} \textbf{x} \in \mathbb{R}$$</p>
<p>为了将所得结果转换为 [0,1] 区间内的概率，逻辑回归使用下列函数进行转换： </p>
<p>$$\sigma(z) &#x3D; \frac{1}{1 + \exp^{-z}}$$</p>
<p>使用 Matplotlib 库画出上面这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>%matplotlib inline<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigma</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><br>xx = np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1000</span>)<br>plt.plot(xx, [sigma(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xx])<br>plt.xlabel(<span class="hljs-string">&#x27;z&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;sigmoid(z)&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Sigmoid function&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>事件 $X$ 的概率记为 $P(X)$，则比值比 $OR(X)$ 由式 $\frac{P(X)}{1-P(X)}$ 决定，比值比是某一事件是否发生的概率之比。显然，概率和比值比包含同样的信息，不过 $P(X)$ 的范围是 0 到 1，而 $OR(X)$ 的范围是 0 到 $\infty$。如果计算 $OR(X)$ 的对数，那么显然有 $\log{OR(X)} \in \mathbb{R}$，这在 OLS 中有用到。</p>
<p>让我们看看逻辑回归是如何做出预测的：</p>
<p>$$p_+ &#x3D; P\left(y_i &#x3D; 1 \mid \textbf{x}_\text{i}, \textbf{w}\right)$$</p>
<p>现在，假设已经通过某种方式得到了权重 w<strong>w</strong>，即模型已经训练好了，逻辑回归预测的步骤如下：</p>
<p>步骤一 计算：</p>
<p>$$w_{0}+w_{1}x_1 + w_{2}x_2 + … &#x3D; \textbf{w}^\text{T}\textbf{x}$$</p>
<p>等式 $\textbf{w}^\text{T}\textbf{x} &#x3D; 0$ 定义了一个超空间将样本分为两类。</p>
<p>步骤二 计算对数比值比 $OR_{+}$</p>
<p>$$ \log(OR_{+}) &#x3D; \textbf{w}^\text{T}\textbf{x}$$</p>
<p>步骤三 现在已经有了将一个样本分配到「+」分类的概率 $OR_{+}$，可以据此计算 $p_{+}$：</p>
<p>$$ p_{+} &#x3D; \frac{OR_{+}}{1 + OR_{+}} &#x3D; \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} &#x3D; \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} &#x3D; \sigma(\textbf{w}^\text{T}\textbf{x})$$</p>
<p>上式的右边就是 sigmoid 函数。</p>
<p>所以，逻辑回归预测一个样本分配为「+」分类的概率（假定已知模型的特征和权重），这一预测过程是通过对权重向量和特征向量的线性组合进行 sigmoid 变换完成的，公式如下：</p>
<p>$$ p_+(\textbf{x}_\text{i}) &#x3D; P\left(y_i &#x3D; 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) &#x3D; \sigma(\textbf{w}^\text{T}\textbf{x}_\text{i}). $$</p>
<p>下面介绍模型是如何被训练的，我们将再次通过最大似然估计训练模型。</p>
<h3 id="最大似然估计和逻辑回归"><a href="#最大似然估计和逻辑回归" class="headerlink" title="最大似然估计和逻辑回归"></a>最大似然估计和逻辑回归</h3><p>现在，看下从最大似然估计（MLE）出发如何进行逻辑回归优化，也就是最小化逻辑损失函数。前面已经见过了将样本分配为「+」分类的逻辑回归模型：</p>
<p>$$ p_+(\textbf{x}_\text{i}) &#x3D; P\left(y_i &#x3D; 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) &#x3D; \sigma(\textbf{w}^T\textbf{x}_\text{i})$$</p>
<p>「-」分类相应的表达式为：</p>
<p>$$ p_-(\textbf{x}_\text{i})  &#x3D; P\left(y_i &#x3D; -1 \mid \textbf{x}_\text{i}, \textbf{w}\right)  &#x3D; 1 - \sigma(\textbf{w}^T\textbf{x}_\text{i}) &#x3D; \sigma(-\textbf{w}^T\textbf{x}_\text{i}) $$</p>
<p>这两个表达式可以组合成一个：</p>
<p>$$ P\left(y &#x3D; y_i \mid \textbf{x}_\text{i}, \textbf{w}\right) &#x3D; \sigma(y_i\textbf{w}^T\textbf{x}_\text{i})$$</p>
<p>表达式 $M(\textbf{x}_\text{i}) &#x3D; y_i\textbf{w}^T\textbf{x}_\text{i}$ 称为目标 $\textbf{x}_\text{i}$ 的分类边缘。如果边缘非负，则模型正确选择了目标 $\textbf{x}_\text{i}$ 的分类；如果边缘为负，则目标  $\textbf{x}_\text{i}$ 被错误分类了。注意，边缘仅针对训练集中的目标（即标签 $y_i$ 已知的目标）而言。</p>
<p>为了准确地理解为何有这一结论，需要理解向线性分类器的几何解释。首先，看下线性代数的一个经典入门问题「找出向径 $\textbf{x}_A$ 与平面 $\textbf{w}^\text{T}\textbf{x} &#x3D; 0$ 的距离」，即：</p>
<p>$$\rho(\textbf{x}_A, \textbf{w}^\text{T}\textbf{x} &#x3D; 0) &#x3D; \frac{\textbf{w}^\text{T}\textbf{x}_A}{||\textbf{w}||}$$</p>
<p>答案：</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175344202.png" srcset="/img/loading.gif" lazyload class title="img">

<p>从答案中，可以看到，表达式 $\textbf{w}^\text{T}\textbf{x}_\text{i}$ 的绝对值越大，点 $\textbf{x}_\text{i}$ 离平面 $\textbf{w}^\text{T}\textbf{x} &#x3D; 0$ 的距离就越远。</p>
<p>因此，表达式 $M(\textbf{x}_\text{i}) &#x3D; y_i\textbf{w}^\text{T}\textbf{x}_\text{i}$ 是模型对目标 $\textbf{x}_\text{i}$ 分类的肯定程度：</p>
<ul>
<li>如果边缘的绝对值较大，且为正值，那么分类的标签是正确的，且目标离分界超平面很远，也就是模型对这一分类很肯定。如下图点 $x_3$ 所示。</li>
<li>如果边缘的绝对值较大，且为负值，那么分类的标签是错误的，且目标离分界超平面很远，那么目标很可能是一个异常值（例如，它可能为训练集中一个错误标记的值）。如下图点 $x_1$ 所示。</li>
<li>如果边缘绝对值较小，那么目标距离分界超平面很近，其符号就决定了目标「是否被正确分类」。如下图点 $x_2$ 和 $x_4$ 所示。</li>
</ul>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175345600.png" srcset="/img/loading.gif" lazyload class title="img">

<p>现在，计算数据集的似然，即基于数据集 $\textbf{x}$ 观测到给定向量 $\textbf{y}$ 的概率。假设目标来自一个独立分布，然后可建立如下公式：</p>
<p>$$ P\left(\textbf{y} \mid \textbf{X}, \textbf{w}\right) &#x3D; \prod_{i&#x3D;1}^{\ell} P\left(y &#x3D; y_i \mid \textbf{x}_\text{i}, \textbf{w}\right),$$</p>
<p>其中，ℓℓ 为数据集 X<strong>X</strong> 的长度（行数）。</p>
<p>对这个表达式取对数，简化计算：</p>
<p>$$ \log P\left(\textbf{y} \mid \textbf{X}, \textbf{w}\right) &#x3D; \log \prod_{i&#x3D;1}^{\ell} P\left(y &#x3D; y_i \mid \textbf{x}<em>\text{i}, \textbf{w}\right) &#x3D; \log \prod</em>{i&#x3D;1}^{\ell} \sigma(y_i\textbf{w}^\text{T}\textbf{x}_\text{i})   &#x3D; $$</p>
<p>$$  &#x3D; \sum_{i&#x3D;1}^{\ell} \log \sigma(y_i\textbf{w}^\text{T}\textbf{x}<em>\text{i}) &#x3D; \sum</em>{i&#x3D;1}^{\ell} \log \frac{1}{1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}<em>\text{i}}} &#x3D; - \sum</em>{i&#x3D;1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}})$$</p>
<p>最大化似然等价于最小化以下表达式：</p>
<p>$$ \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}) &#x3D; \sum_{i&#x3D;1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}).$$</p>
<p>上式就是逻辑损失函数。用分类边缘 $M(\textbf{x}_\text{i})$ 改写逻辑损失函数，有 $L(M) &#x3D; \log (1 + \exp^{-M})$</p>
<p>将这一函数的图像和 0-1 损失函数的图像绘制在一张图上。当错误分类发生时，0-1 损失函数只会以恒定的数值 1.0 惩罚模型，即 $L_{1&#x2F;0}(M) &#x3D; [M &lt; 0]$。</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175346295.png" srcset="/img/loading.gif" lazyload class title="img">

<p>上图体现了这样一个想法：如果不能够直接最小化分类问题的误差数量（至少无法通过梯度方法最小化，因为 0-1 损失函数在 0 的导数趋向无穷），那么可以转而最小化它的上界。对逻辑损失函数而言，以下公式是成立的：</p>
<p>$$ \mathcal{L_{1&#x2F;0}} (\textbf X, \textbf{y}, \textbf{w}) &#x3D; \sum_{i&#x3D;1}^{\ell} [M(\textbf{x}<em>\text{i}) &lt; 0] \leq \sum</em>{i&#x3D;1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}<em>\text{i}}) &#x3D; \mathcal{L</em>{\log}} (\textbf X, \textbf{y}, \textbf{w}), $$</p>
<p>其中 $\mathcal{L_{1&#x2F;0}} (\textbf X, \textbf{y})$ 只是数据集$（\textbf X, \textbf{y}）$ 上权重 $\textbf{w}$ 的逻辑回归误差。因此，可以通过降低分类误差数 $\mathcal{L_{log}}$ 的上限，降低分数误差数本身。</p>
<h3 id="逻辑回归的-l-2-正则化"><a href="#逻辑回归的-l-2-正则化" class="headerlink" title="逻辑回归的 $l_2$正则化"></a>逻辑回归的 $l_2$正则化</h3><p>逻辑回归的 L2 正则化和岭回归的情况基本一样。代替 $\mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w})$，只用最小化下式：</p>
<p>$$ \mathcal{J}(\textbf X, \textbf{y}, \textbf{w}) &#x3D; \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}) + \lambda |\textbf{w}|^2$$</p>
<p>在逻辑回归中，通常使用正则化系数的倒数 $C &#x3D; \frac{1}{\lambda}$：</p>
<p>$$ \widehat{\textbf w}  &#x3D; \arg \min_{\textbf{w}} \mathcal{J}(\textbf X, \textbf{y}, \textbf{w}) &#x3D;  \arg \min_{\textbf{w}}\ (C\sum_{i&#x3D;1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}})+ |\textbf{w}|^2)$$ </p>
<p>下面通过一个例子直观地理解正则化。</p>
<h3 id="逻辑回归正则化示例"><a href="#逻辑回归正则化示例" class="headerlink" title="逻辑回归正则化示例"></a>逻辑回归正则化示例</h3><p>正则化是如何影响分类质量的呢？我们使用吴恩达机器学习课程中的「微芯片测试」数据集，运用基于多项式特征的逻辑回归方法，然后改变正则化参数 𝐶<em>C</em>。首先，看看正则化是如何影响分类器的分界，并查看欠拟合和过拟合的情况。接着，将通过交叉验证和网格搜索方法来选择接近最优值的正则化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score, StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression, LogisticRegressionCV<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>%matplotlib inline<br><br></code></pre></td></tr></table></figure>

<p>使用 Pandas 库的 <code>read_csv()</code> 方法加载数据。这个数据集内有 118 个微芯片（目标），其中有两项质量控制测试的结果（两个数值变量）和微芯片是否投产的信息。变量已经过归一化，即列中的值已经减去其均值。所以，微芯片的平均测试值为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span><br>data = pd.read_csv(<span class="hljs-string">&#x27;https://labfile.oss.aliyuncs.com/courses/1283/microchip_tests.txt&#x27;</span>,<br>                   header=<span class="hljs-literal">None</span>, names=(<span class="hljs-string">&#x27;test1&#x27;</span>, <span class="hljs-string">&#x27;test2&#x27;</span>, <span class="hljs-string">&#x27;released&#x27;</span>))<br><span class="hljs-comment"># 查看数据集的一些信息</span><br>data.info()<br></code></pre></td></tr></table></figure>

<p>查看开始五行和最后五行的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">data.head(<span class="hljs-number">5</span>)<br><br>data.tail(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure>

<p>分离训练集和目标分类标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X = data.iloc[:, :<span class="hljs-number">2</span>].values<br>y = data.iloc[:, <span class="hljs-number">2</span>].values<br></code></pre></td></tr></table></figure>

<p>绘制数据，橙点对应有缺陷的芯片，蓝点对应正常芯片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&#x27;Released&#x27;</span>)<br>plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&#x27;Faulty&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Test 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Test 2&quot;</span>)<br>plt.title(<span class="hljs-string">&#x27;2 tests of microchips. Logit with C=1&#x27;</span>)<br>plt.legend()<br></code></pre></td></tr></table></figure>

<p>定义一个函数来显示分类器的分界线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_boundary</span>(<span class="hljs-params">clf, X, y, grid_step=<span class="hljs-number">.01</span>, poly_featurizer=<span class="hljs-literal">None</span></span>):<br>    x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">.1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">.1</span><br>    y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">.1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">.1</span><br>    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),<br>                         np.arange(y_min, y_max, grid_step))<br><br>    <span class="hljs-comment"># 在 [x_min, m_max]x[y_min, y_max] 的每一点都用它自己的颜色来对应</span><br>    Z = clf.predict(poly_featurizer.transform(np.c_[xx.ravel(), yy.ravel()]))<br>    Z = Z.reshape(xx.shape)<br>    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)<br></code></pre></td></tr></table></figure>

<p>为两个变量 𝑥1<em>x</em>1 和 𝑥2<em>x</em>2 定义如下多形式特征：</p>
<p>$$ {x_1^d, x_1^{d-1}x_2, \ldots x_2^d} &#x3D;  {x_1^ix_2^j}_{i+j&#x3D;d, i,j \in \mathbb{N}}$$</p>
<p>例如，𝑑&#x3D;3<em>d</em>&#x3D;3 时的特征如下：</p>
<p>$$ 1, x_1, x_2,  x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3$$</p>
<p>特征的数量会呈指数型增长，为 100 个变量创建 d 较大（例如 𝑑&#x3D;10<em>d</em>&#x3D;10）的多项式特征会导致计算成本变得很高。</p>
<p>使用 sklearn 库来实现逻辑回归。创建一个对象，为矩阵 𝑋<em>X</em> 加上多项式特征（𝑑<em>d</em> 不超过7）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">poly = PolynomialFeatures(degree=<span class="hljs-number">7</span>)<br>X_poly = poly.fit_transform(X)<br><br>X_poly.shape<br></code></pre></td></tr></table></figure>

<p>训练逻辑回归模型，正则化系数 𝐶&#x3D;10−2<em>C</em>&#x3D;10−2。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">C = <span class="hljs-number">1e-2</span><br>logit = LogisticRegression(C=C, random_state=<span class="hljs-number">17</span>)<br>logit.fit(X_poly, y)<br><br>plot_boundary(logit, X, y, grid_step=<span class="hljs-number">.01</span>, poly_featurizer=poly)<br><br>plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&#x27;Released&#x27;</span>)<br>plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&#x27;Faulty&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Test 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Test 2&quot;</span>)<br>plt.title(<span class="hljs-string">&#x27;2 tests of microchips. Logit with C=%s&#x27;</span> % C)<br>plt.legend()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy on training set:&quot;</span>,<br>      <span class="hljs-built_in">round</span>(logit.score(X_poly, y), <span class="hljs-number">3</span>))<br><br></code></pre></td></tr></table></figure>

<p>可以尝试减小正则化，即把 𝐶<em>C</em> 增加到 1，现在的模型权重可以比之前有更大的值（绝对值更大），这使得分类器在训练集上的精确度提高到 0.831。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">C = <span class="hljs-number">1</span><br>logit = LogisticRegression(C=C, random_state=<span class="hljs-number">17</span>)<br>logit.fit(X_poly, y)<br><br>plot_boundary(logit, X, y, grid_step=<span class="hljs-number">.005</span>, poly_featurizer=poly)<br><br>plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&#x27;Released&#x27;</span>)<br>plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&#x27;Faulty&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Test 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Test 2&quot;</span>)<br>plt.title(<span class="hljs-string">&#x27;2 tests of microchips. Logit with C=%s&#x27;</span> % C)<br>plt.legend()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy on training set:&quot;</span>,<br>      <span class="hljs-built_in">round</span>(logit.score(X_poly, y), <span class="hljs-number">3</span>))<br><br></code></pre></td></tr></table></figure>

<p>倘若继续增加 𝐶<em>C</em> 到 10000 会如何？看下面结果，很明显正则化不够强导致了过拟合现象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">C = <span class="hljs-number">1e4</span><br>logit = LogisticRegression(C=C, random_state=<span class="hljs-number">17</span>)<br>logit.fit(X_poly, y)<br><br>plot_boundary(logit, X, y, grid_step=<span class="hljs-number">.005</span>, poly_featurizer=poly)<br><br>plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&#x27;Released&#x27;</span>)<br>plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&#x27;Faulty&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Test 1&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Test 2&quot;</span>)<br>plt.title(<span class="hljs-string">&#x27;2 tests of microchips. Logit with C=%s&#x27;</span> % C)<br>plt.legend()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy on training set:&quot;</span>,<br>      <span class="hljs-built_in">round</span>(logit.score(X_poly, y), <span class="hljs-number">3</span>))<br><br></code></pre></td></tr></table></figure>

<p>为了讨论上述的这些结果，改写一下逻辑回归的优化函数：</p>
<p>$$ J(X,y,w) &#x3D; \mathcal{L} + \frac{1}{C}||w||^2,$$</p>
<p>其中，</p>
<ul>
<li>$\mathcal{L}$ 是对整个数据集的总逻辑损失函数</li>
<li>$C$ 是反向正则化系数</li>
</ul>
<p>总结：</p>
<ul>
<li>参数 $C$ 越大，模型中可恢复的数据之间的关系就越复杂（直观地说，$C$ 对应模型的「复杂度」：模型能力）。</li>
<li>如果正则化过强，即 $C$ 值很小，最小化逻辑损失函数问题的解（权重）可能过小或为零。这样的模型对误差的「惩罚」也不够（即在上面的函数 $J(X,y,w)$ 中，权重的平方和过高，导致误差 $L$ 较大）。在这一情形下，模型将会欠拟合。</li>
<li>如果正则化过弱，即 $C$ 值很大，最小化逻辑损失函数问题的解可能权重过大。在这一情形下， $L$ 对函数 $J(X,y,w)$ 贡献较大，导致过拟合。</li>
<li>$C$ 是一个超参数，逻辑回归不会自动学习调整 $C$ 的值，我们可以通过交叉验证来人工选择较好的 $C$ 值。</li>
</ul>
<h3 id="正则化参数的调整"><a href="#正则化参数的调整" class="headerlink" title="正则化参数的调整"></a>正则化参数的调整</h3><p>对上述例子中正则化参数 $C$ 进行调参。使用 <code>LogisticRegressionCV()</code> 方法进行网格搜索参数后再交叉验证，<code>LogisticRegressionCV()</code> 是专门为逻辑回归设计的。如果想对其他模型进行同样的操作，可以使用 <code>GridSearchCV()</code> 或 <code>RandomizedSearchCV()</code> 等超参数优化算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 该单元格执行时间较长，请耐心等待</span><br>skf = StratifiedKFold(n_splits=<span class="hljs-number">5</span>, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">17</span>)<br><span class="hljs-comment"># 下方结尾的切片为了在线上环境搜索更快，线下练习时可以删除</span><br>c_values = np.logspace(-<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">500</span>)[<span class="hljs-number">50</span>:<span class="hljs-number">450</span>:<span class="hljs-number">50</span>]<br><br>logit_searcher = LogisticRegressionCV(<br>    Cs=c_values, cv=skf, verbose=<span class="hljs-number">1</span>, n_jobs=-<span class="hljs-number">1</span>)<br>logit_searcher.fit(X_poly, y)<br><br>logit_searcher.C_<br><br></code></pre></td></tr></table></figure>

<p>查看超参数 $C$ 是如何影响模型的质量的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(c_values, np.mean(logit_searcher.scores_[<span class="hljs-number">1</span>], axis=<span class="hljs-number">0</span>))<br>plt.xlabel(<span class="hljs-string">&#x27;C&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Mean CV-accuracy&#x27;</span>)<br><br></code></pre></td></tr></table></figure>

<p>最后，选择 $C$ 值「最佳」的区域，即在 Mean CV-accuracy 值达到较大值的前提下选择较小的 $C$。上图由于 $C$ 过大，无法辨认具体哪个较小的 $C$ 达到了较好的 Mean CV-accuracy 值，可以仅画出 $C$ 为 0 到 10 时的验证曲线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(c_values, np.mean(logit_searcher.scores_[<span class="hljs-number">1</span>], axis=<span class="hljs-number">0</span>))<br>plt.xlabel(<span class="hljs-string">&#x27;C&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Mean CV-accuracy&#x27;</span>)<br>plt.xlim((<span class="hljs-number">0</span>, <span class="hljs-number">10</span>))<br><br></code></pre></td></tr></table></figure>

<p>上图可见，$C&#x3D;2$ 时就达到了较好的 Mean CV-accuracy 值。</p>
<h3 id="逻辑回归的优缺点"><a href="#逻辑回归的优缺点" class="headerlink" title="逻辑回归的优缺点"></a>逻辑回归的优缺点</h3><p>通过分析 IMDB 影评的二元分类问题和 XOR 问题来简要说明逻辑回归的优缺点。</p>
<h4 id="分析-IMDB-二元分类问题"><a href="#分析-IMDB-二元分类问题" class="headerlink" title="分析 IMDB 二元分类问题"></a>分析 IMDB 二元分类问题</h4><p>IMDB 数据集中的训练集包含标记好的影评，其中有 12500 条好评，12500 条差评。使用词袋模型构建输入矩阵 𝑋<em>X</em> ，语料库包含所有用户影评，影评的特征将由整个语料库中每个词的出现情况来表示。下图展示了这一思路：</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175347318.svg+xml" srcset="/img/loading.gif" lazyload class title="img">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_files<br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br></code></pre></td></tr></table></figure>

<p>载入 IMDB 数据集。首先，我们从蓝桥云课服务器上下载并解压数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 文件较多，请耐心等待解压完成</span><br>!wget -nc <span class="hljs-string">&quot;https://labfile.oss.aliyuncs.com/courses/1283/aclImdb_v1.tar.gz&quot;</span><br>!tar -zxvf <span class="hljs-string">&quot;aclImdb_v1.tar.gz&quot;</span><br>PATH_TO_IMDB = <span class="hljs-string">&quot;aclImdb/&quot;</span><br>reviews_train = load_files(os.path.join(PATH_TO_IMDB, <span class="hljs-string">&quot;train&quot;</span>),<br>                           categories=[<span class="hljs-string">&#x27;pos&#x27;</span>, <span class="hljs-string">&#x27;neg&#x27;</span>])<br>text_train, y_train = reviews_train.data, reviews_train.target<br>reviews_test = load_files(os.path.join(PATH_TO_IMDB, <span class="hljs-string">&quot;test&quot;</span>),<br>                          categories=[<span class="hljs-string">&#x27;pos&#x27;</span>, <span class="hljs-string">&#x27;neg&#x27;</span>])<br>text_test, y_test = reviews_test.data, reviews_test.target<br><br></code></pre></td></tr></table></figure>

<p>看看训练集和测试集中各有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of documents in training data: %d&quot;</span> % <span class="hljs-built_in">len</span>(text_train))<br><span class="hljs-built_in">print</span>(np.bincount(y_train))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of documents in test data: %d&quot;</span> % <span class="hljs-built_in">len</span>(text_test))<br><span class="hljs-built_in">print</span>(np.bincount(y_test))<br><br></code></pre></td></tr></table></figure>

<p>下面是该数据集中的一些影评。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">text_train[<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure>

<p>查看一下上面这条影评是差评还是好评。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y_train[<span class="hljs-number">1</span>]<br><br></code></pre></td></tr></table></figure>

<p>y_train&#x3D;0 表示影评是差评，y_train&#x3D;1 表示影评是好评，上面这条影片是差评。</p>
<h4 id="单词的简单计数"><a href="#单词的简单计数" class="headerlink" title="单词的简单计数"></a>单词的简单计数</h4><p>首先，使用 <code>CountVectorizer()</code> 创建包含所有单词的字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">cv = CountVectorizer()<br>cv.fit(text_train)<br><br><span class="hljs-built_in">len</span>(cv.vocabulary_)<br></code></pre></td></tr></table></figure>

<p>查看创建后的「单词」样本，发现 IMDB 数据集已经自动进行了文本处理（自动化文本处理不在本实验讨论范围，如果感兴趣可以自行搜索）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(cv.get_feature_names()[:<span class="hljs-number">50</span>])<br><span class="hljs-built_in">print</span>(cv.get_feature_names()[<span class="hljs-number">50000</span>:<span class="hljs-number">50050</span>])<br></code></pre></td></tr></table></figure>

<p>接着，使用单词的索引编码训练集的句子，用稀疏矩阵保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train = cv.transform(text_train)<br>X_train<br></code></pre></td></tr></table></figure>

<p>让我们看看上述转换过程是如何进行的，首先查看需要转换的训练集句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">text_train[<span class="hljs-number">19726</span>]<br></code></pre></td></tr></table></figure>

<p>然后将每个单词转换成对应的单词索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train[<span class="hljs-number">19726</span>].nonzero()[<span class="hljs-number">1</span>]<br><br>X_train[<span class="hljs-number">19726</span>].nonzero()<br></code></pre></td></tr></table></figure>

<p>接下来，对测试集应用同样的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_test = cv.transform(text_test)<br></code></pre></td></tr></table></figure>

<p>之后就可以使用逻辑回归来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">logit = LogisticRegression(solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>, n_jobs=-<span class="hljs-number">1</span>, random_state=<span class="hljs-number">7</span>)<br>logit.fit(X_train, y_train)<br></code></pre></td></tr></table></figure>

<p>训练完成后，查看训练集和测试集上的准确率（Accuracy）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">round</span>(logit.score(X_train, y_train), <span class="hljs-number">3</span>), <span class="hljs-built_in">round</span>(logit.score(X_test, y_test), <span class="hljs-number">3</span>),<br></code></pre></td></tr></table></figure>

<p>可视化模型的系数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_coefficients</span>(<span class="hljs-params">classifier, feature_names, n_top_features=<span class="hljs-number">25</span></span>):<br>    <span class="hljs-comment"># get coefficients with large absolute values</span><br>    coef = classifier.coef_.ravel()<br>    positive_coefficients = np.argsort(coef)[-n_top_features:]<br>    negative_coefficients = np.argsort(coef)[:n_top_features]<br>    interesting_coefficients = np.hstack(<br>        [negative_coefficients, positive_coefficients])<br>    <span class="hljs-comment"># plot them</span><br>    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">5</span>))<br>    colors = [<span class="hljs-string">&quot;red&quot;</span> <span class="hljs-keyword">if</span> c &lt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;blue&quot;</span> <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> coef[interesting_coefficients]]<br>    plt.bar(np.arange(<span class="hljs-number">2</span> * n_top_features),<br>            coef[interesting_coefficients], color=colors)<br>    feature_names = np.array(feature_names)<br>    plt.xticks(np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">1</span> + <span class="hljs-number">2</span> * n_top_features),<br>               feature_names[interesting_coefficients], rotation=<span class="hljs-number">60</span>, ha=<span class="hljs-string">&quot;right&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_grid_scores</span>(<span class="hljs-params">grid, param_name</span>):<br>    plt.plot(grid.param_grid[param_name], grid.cv_results_[<span class="hljs-string">&#x27;mean_train_score&#x27;</span>],<br>             color=<span class="hljs-string">&#x27;green&#x27;</span>, label=<span class="hljs-string">&#x27;train&#x27;</span>)<br>    plt.plot(grid.param_grid[param_name], grid.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>],<br>             color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&#x27;test&#x27;</span>)<br>    plt.legend()<br><br>visualize_coefficients(logit, cv.get_feature_names())<br><br></code></pre></td></tr></table></figure>

<p>对逻辑回归的正则化系数进行调参。<code>make_pipeline()</code> 确保的序列顺序，在训练数据上应用 <code>CountVectorizer()</code> 方法，然后训练逻辑回归模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline<br><span class="hljs-comment"># 该单元格执行时间较长，请耐心等待</span><br>text_pipe_logit = make_pipeline(CountVectorizer(),<br>                                LogisticRegression(solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>,<br>                                                   n_jobs=<span class="hljs-number">1</span>,<br>                                                   random_state=<span class="hljs-number">7</span>))<br><br>text_pipe_logit.fit(text_train, y_train)<br><span class="hljs-built_in">print</span>(text_pipe_logit.score(text_test, y_test))<br><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br><span class="hljs-comment"># 该单元格执行时间较长，请耐心等待</span><br>param_grid_logit = &#123;<span class="hljs-string">&#x27;logisticregression__C&#x27;</span>: np.logspace(-<span class="hljs-number">5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>)[<span class="hljs-number">4</span>:<span class="hljs-number">5</span>]&#125;<br>grid_logit = GridSearchCV(text_pipe_logit,<br>                          param_grid_logit,<br>                          return_train_score=<span class="hljs-literal">True</span>,<br>                          cv=<span class="hljs-number">3</span>, n_jobs=-<span class="hljs-number">1</span>)<br><br>grid_logit.fit(text_train, y_train)<br><br></code></pre></td></tr></table></figure>

<p>查看一下最佳 $C$，以及相应的交叉验证评分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">grid_logit.best_params_, grid_logit.best_score_<br><br>plot_grid_scores(grid_logit, <span class="hljs-string">&#x27;logisticregression__C&#x27;</span>)<br><br></code></pre></td></tr></table></figure>

<p>调优后的逻辑回归模型在验证集上的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">grid_logit.score(text_test, y_test)<br><br></code></pre></td></tr></table></figure>

<p>现在换一种方法，使用随机森林来分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>forest = RandomForestClassifier(n_estimators=<span class="hljs-number">200</span>,<br>                                n_jobs=-<span class="hljs-number">1</span>, random_state=<span class="hljs-number">17</span>)<br><br>forest.fit(X_train, y_train)<br><br><span class="hljs-built_in">round</span>(forest.score(X_test, y_test), <span class="hljs-number">3</span>)<br><br></code></pre></td></tr></table></figure>

<p>上述结果可见，相较于随机森林，逻辑回归在 IMDB 数据集上表现更优。</p>
<h4 id="XOR-问题"><a href="#XOR-问题" class="headerlink" title="XOR 问题"></a>XOR 问题</h4><p>线性分类定义的是一个非常简单的分界平面：一个超平面，这导致线性模型在 XOR 问题上表现不佳。XOR 即异或，其真值表如下：</p>
<img src="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%AF%BC%E8%AF%BB/IMG-20240826175347669.gif" srcset="/img/loading.gif" lazyload class title="img">

<p>XOR 是一个简单的二元分类问题，其中两个分类呈对角交叉分布。下面创建数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">rng = np.random.RandomState(<span class="hljs-number">0</span>)<br>X = rng.randn(<span class="hljs-number">200</span>, <span class="hljs-number">2</span>)<br>y = np.logical_xor(X[:, <span class="hljs-number">0</span>] &gt; <span class="hljs-number">0</span>, X[:, <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>)<br><br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, c=y, cmap=plt.cm.Paired)<br><br></code></pre></td></tr></table></figure>

<p>显然，无法划出一条直线无误差地将两个分类分开。因此，逻辑回归在这一任务上的表现很差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_boundary</span>(<span class="hljs-params">clf, X, y, plot_title</span>):<br>    xx, yy = np.meshgrid(np.linspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">50</span>),<br>                         np.linspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">50</span>))<br>    clf.fit(X, y)<br>    <span class="hljs-comment"># plot the decision function for each datapoint on the grid</span><br>    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, <span class="hljs-number">1</span>]<br>    Z = Z.reshape(xx.shape)<br><br>    image = plt.imshow(Z, interpolation=<span class="hljs-string">&#x27;nearest&#x27;</span>,<br>                       extent=(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>(), yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>()),<br>                       aspect=<span class="hljs-string">&#x27;auto&#x27;</span>, origin=<span class="hljs-string">&#x27;lower&#x27;</span>, cmap=plt.cm.PuOr_r)<br>    contours = plt.contour(xx, yy, Z, levels=[<span class="hljs-number">0</span>], linewidths=<span class="hljs-number">2</span>,<br>                           linetypes=<span class="hljs-string">&#x27;--&#x27;</span>)<br>    plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, c=y, cmap=plt.cm.Paired)<br>    plt.xticks(())<br>    plt.yticks(())<br>    plt.xlabel(<span class="hljs-string">r&#x27;$x_1$&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">r&#x27;$x_2$&#x27;</span>)<br>    plt.axis([-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>    plt.colorbar(image)<br>    plt.title(plot_title, fontsize=<span class="hljs-number">12</span>)<br><br>plot_boundary(LogisticRegression(solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>), X, y,<br>              <span class="hljs-string">&quot;Logistic Regression, XOR problem&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>然而，如果将输入变为多项式特征（这里 𝑑<em>d</em> &#x3D; 2），那么这一任务就可以得到较好的解决。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures<br><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline<br><br>logit_pipe = Pipeline([(<span class="hljs-string">&#x27;poly&#x27;</span>, PolynomialFeatures(degree=<span class="hljs-number">2</span>)),<br>                       (<span class="hljs-string">&#x27;logit&#x27;</span>, LogisticRegression(solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>))])<br><br>plot_boundary(logit_pipe, X, y,<br>              <span class="hljs-string">&quot;Logistic Regression + quadratic features. XOR problem&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>通过将多项式特征作为输入，逻辑回归在 6 维特征空间（1，𝑥1，𝑥2，𝑥12，𝑥1𝑥2，𝑥221，<em>x</em>1，<em>x</em>2，<em>x</em>12，<em>x</em>1<em>x</em>2，<em>x</em>22）中生成了一个超平面。当这个超平面投影到原特征空间（𝑥1,𝑥2<em>x</em>1,<em>x</em>2）时，分界是非线性的。</p>
<p>在实际应用中，多项式特征确实有用，不过显式的创建它们会大大提升计算复杂度。使用核（kernel）函数的 SVM 方法相较逻辑回归要快很多，在 SVM 中，只计算高维空间中目标之间的距离（由核函数定义），而不用生成大量特征组合。</p>
<h3 id="验证和学习曲线"><a href="#验证和学习曲线" class="headerlink" title="验证和学习曲线"></a>验证和学习曲线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures<br><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression, LogisticRegressionCV, SGDClassifier<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> validation_curve, learning_curve<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure>

<p>上文对模型验证、交叉验证、正则化做了简单介绍，现在考虑一个更大的问题：如果模型的质量不佳，该怎么办？针对这个问题，有很多猜想：</p>
<ul>
<li>应该让模型更复杂还是更简单？</li>
<li>应该加入更多特征吗？</li>
<li>是否只是需要更多数据用于训练？</li>
</ul>
<p>这些猜想的答案并不明显，比如有时候一个更复杂的模型会导致表现退化，有时候增加新的特征带来的变化并不直观。事实上，做出正确决定，选择正确方法，从而改进模型的能力是衡量一个人对机器学习知识掌握程度的重要指标。</p>
<p>让我们回头看看电信运营商的客户离网数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">data = pd.read_csv(<br>    <span class="hljs-string">&#x27;https://labfile.oss.aliyuncs.com/courses/1283/telecom_churn.csv&#x27;</span>).drop(<span class="hljs-string">&#x27;State&#x27;</span>, axis=<span class="hljs-number">1</span>)<br>data[<span class="hljs-string">&#x27;International plan&#x27;</span>] = data[<span class="hljs-string">&#x27;International plan&#x27;</span>].<span class="hljs-built_in">map</span>(<br>    &#123;<span class="hljs-string">&#x27;Yes&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;No&#x27;</span>: <span class="hljs-number">0</span>&#125;)<br>data[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>] = data[<span class="hljs-string">&#x27;Voice mail plan&#x27;</span>].<span class="hljs-built_in">map</span>(&#123;<span class="hljs-string">&#x27;Yes&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;No&#x27;</span>: <span class="hljs-number">0</span>&#125;)<br><br>y = data[<span class="hljs-string">&#x27;Churn&#x27;</span>].astype(<span class="hljs-string">&#x27;int&#x27;</span>).values<br>X = data.drop(<span class="hljs-string">&#x27;Churn&#x27;</span>, axis=<span class="hljs-number">1</span>).values<br><br></code></pre></td></tr></table></figure>

<p>使用随机梯度下降训练逻辑回归（在之后的实验中将会专门讨论梯度下降）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">alphas = np.logspace(-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">20</span>)<br>sgd_logit = SGDClassifier(loss=<span class="hljs-string">&#x27;log&#x27;</span>, n_jobs=-<span class="hljs-number">1</span>, random_state=<span class="hljs-number">17</span>, max_iter=<span class="hljs-number">5</span>)<br>logit_pipe = Pipeline([(<span class="hljs-string">&#x27;scaler&#x27;</span>, StandardScaler()), (<span class="hljs-string">&#x27;poly&#x27;</span>, PolynomialFeatures(degree=<span class="hljs-number">2</span>)),<br>                       (<span class="hljs-string">&#x27;sgd_logit&#x27;</span>, sgd_logit)])<br>val_train, val_test = validation_curve(logit_pipe, X, y,<br>                                       <span class="hljs-string">&#x27;sgd_logit__alpha&#x27;</span>, alphas, cv=<span class="hljs-number">5</span>,<br>                                       scoring=<span class="hljs-string">&#x27;roc_auc&#x27;</span>)<br><br></code></pre></td></tr></table></figure>

<p>绘制 ROC-AUC 曲线，查看不同正则化参数下模型在训练集和测试集上的表现有何不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_with_err</span>(<span class="hljs-params">x, data, **kwargs</span>):<br>    mu, std = data.mean(<span class="hljs-number">1</span>), data.std(<span class="hljs-number">1</span>)<br>    lines = plt.plot(x, mu, <span class="hljs-string">&#x27;-&#x27;</span>, **kwargs)<br>    plt.fill_between(x, mu - std, mu + std, edgecolor=<span class="hljs-string">&#x27;none&#x27;</span>,<br>                     facecolor=lines[<span class="hljs-number">0</span>].get_color(), alpha=<span class="hljs-number">0.2</span>)<br><br><br>plot_with_err(alphas, val_train, label=<span class="hljs-string">&#x27;training scores&#x27;</span>)<br>plot_with_err(alphas, val_test, label=<span class="hljs-string">&#x27;validation scores&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">r&#x27;$\alpha$&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ROC AUC&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br><br></code></pre></td></tr></table></figure>

<p>上图的趋势表明：</p>
<ul>
<li>简单模型的训练误差和验证误差很接近，且都比较大。这暗示模型欠拟合，参数数量不够多。</li>
<li>高度复杂模型的训练误差和验证误差相差很大，这暗示模型过拟合。当参数数量过多或者正则化不够严格时，算法可能被数据中的噪声「转移注意力」，没能把握数据的整体趋势。</li>
</ul>
<p>上述结论可以推广到其他问题中。</p>
<h3 id="数据对于模型的影响"><a href="#数据对于模型的影响" class="headerlink" title="数据对于模型的影响"></a>数据对于模型的影响</h3><p>一般而言，模型所用的数据越多越好。但新数据是否在任何情况下都有帮助呢？例如，为了评估特征 N ，而对数据集的数据进行加倍，这样做是否合理？</p>
<p>由于新数据可能难以取得，合理的做法是改变训练集的大小，然后看模型的质量与训练数据的数量之间的依赖关系，这就是「学习曲线」的概念。</p>
<p>这个想法很简单：将误差看作训练中所使用的样本数量的函数。模型的参数事先固定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_learning_curve</span>(<span class="hljs-params">degree=<span class="hljs-number">2</span>, alpha=<span class="hljs-number">0.01</span></span>):<br>    train_sizes = np.linspace(<span class="hljs-number">0.05</span>, <span class="hljs-number">1</span>, <span class="hljs-number">20</span>)<br>    logit_pipe = Pipeline([(<span class="hljs-string">&#x27;scaler&#x27;</span>, StandardScaler()), (<span class="hljs-string">&#x27;poly&#x27;</span>, PolynomialFeatures(degree=degree)),<br>                           (<span class="hljs-string">&#x27;sgd_logit&#x27;</span>, SGDClassifier(n_jobs=-<span class="hljs-number">1</span>, random_state=<span class="hljs-number">17</span>, alpha=alpha, max_iter=<span class="hljs-number">5</span>))])<br>    N_train, val_train, val_test = learning_curve(logit_pipe,<br>                                                  X, y, train_sizes=train_sizes, cv=<span class="hljs-number">5</span>,<br>                                                  scoring=<span class="hljs-string">&#x27;roc_auc&#x27;</span>)<br>    plot_with_err(N_train, val_train, label=<span class="hljs-string">&#x27;training scores&#x27;</span>)<br>    plot_with_err(N_train, val_test, label=<span class="hljs-string">&#x27;validation scores&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;Training Set Size&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;AUC&#x27;</span>)<br>    plt.legend()<br>    plt.grid(<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>把正则化系数设定为较大的数（alpha&#x3D;10），查看线性模型的表现情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot_learning_curve(degree=<span class="hljs-number">2</span>, alpha=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure>

<p>上图表明：对于少量数据而言，训练集和交叉验证集之间的误差差别（方差）相当大，这暗示了过拟合。同样的模型，使用大量数据，误差「收敛」，暗示了欠拟合。加入更多数据，该训练集的误差不会增加，且该验证集上的误差也不会下降。所以，倘若误差「收敛」，如果不改变模型的复杂度，而是仅仅把数据集大小增大 10 倍，或许对最终的表现结果没有太大帮助。</p>
<p>如果将正则化系数 alpha 降低到 0.05，会怎么样？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot_learning_curve(degree=<span class="hljs-number">2</span>, alpha=<span class="hljs-number">0.05</span>)<br></code></pre></td></tr></table></figure>

<p>上图表明，降低正则化系数 alpha 至 0.05，曲线将逐渐收敛，如果加入更多数据，可以进一步改善模型在验证集上的表现。</p>
<p>如果把 alpha 设为 $10^{-4}$，让模型更复杂，会出现什么情况？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot_learning_curve(degree=<span class="hljs-number">2</span>, alpha=<span class="hljs-number">1e-4</span>)<br></code></pre></td></tr></table></figure>

<p>上图表明，与正则化系数 alpha&#x3D;0.05 相比，在训练集和验证集上，AUC 都下降了，出现过拟合现象。</p>
<p>构建学习曲线和验证曲线可以帮助我们为新数据调整合适的模型复杂度。</p>
<p>关于验证曲线和学习曲线的结论：</p>
<ul>
<li>训练集上的误差本身不能说明模型的质量。</li>
<li>交叉验证误差除了可以显示模型对数据的拟合程度外，还可以显示模型保留了多少对新数据的概括能力。</li>
<li>验证曲线是一条根据模型复杂度显示训练集和验证集结果的曲线：如果两条曲线彼此接近，且两者的误差都很大，这标志着欠拟合；如果两条曲线彼此距离很远，这标志着过拟合。</li>
<li>学习曲线是一个根据观测数量显示训练集和验证集结果的曲线：如果两条曲线收敛，那么增加新数据收益不大，有必要改变模型复杂度；如果两条曲线没有收敛，增加新数据可以改善结果。</li>
</ul>
<h3 id="实验总结-3"><a href="#实验总结-3" class="headerlink" title="实验总结"></a>实验总结</h3><p>本次实验主要使用逻辑回归的方法构建线性回归和线性分类模型，正则化、验证曲线、学习曲线方法可以帮助我们更好更快的构建模型。</p>
<p> <em>相关链接</em></p>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.deeplearningbook.org/"> <em>《深度学习》</em></a>.</li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://scikit-learn.org/stable/documentation.html"> <em>scikit-learn 文档</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/amueller/scipy-2017-sklearn"> <em>scikit-learn 指南</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/rushter/MLAlgorithms"> <em>适合线性回归和逻辑回归的机器学习算法实现</em></a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.lanqiao.cn/louplus/"> <em>了解蓝桥云课《楼+ 机器学习和数据挖掘课程》</em></a></li>
</ul>

                
              </div>
            
            <hr>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/A-%E5%86%85%E5%8A%9F/" class="category-chain-item">A_内功</a>
  
  
    <span>></span>
    
  <a href="/categories/A-%E5%86%85%E5%8A%9F/C-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">C_机器学习</a>
  
  
    <span>></span>
    
  <a href="/categories/A-%E5%86%85%E5%8A%9F/C-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="category-chain-item">机器学习基础</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%AE%9E%E9%AA%8C%E6%A5%BC/" class="print-no-link">#实验楼</a>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
        <a href="/tags/machine-learning/" class="print-no-link">#machine-learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【机器学习基础】导读</div>
      <div>http://example.com/A_内功/C_机器学习/机器学习基础/【机器学习基础】导读/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>mingming</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年6月17日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/" rel="external nofollow noopener noreferrer">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/A_%E5%86%85%E5%8A%9F/C_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%BA%94%E7%94%A8/" title="【机器学习基础】决策树和随机森林分析应用">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【机器学习基础】决策树和随机森林分析应用</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/A_OS/Windows/Defender/Defender/" title="Windows Defender 相关文章推荐">
                        <span class="hidden-mobile">Windows Defender 相关文章推荐</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="external nofollow noopener noreferrer"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="external nofollow noopener noreferrer"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script>
  <link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css">

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script>
<script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script>
<script src="/js/events.js"></script>
<script src="/js/plugins.js"></script>


  <script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script src="/js/img-lazyload.js"></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script src="/js/local-search.js"></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script src="/js/boot.js"></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
